---
title: "wordcountcheck"
author: "Bergelson et al."
date: "1/11/2018"
output:
  word_document: default
  html_document: default
bibliography:
- sixseven.bib
- r-references.bib
---
```{r read in data, comment=F, message=F, hide = T, warning = F, echo = F}
library("papaja")
library("knitr")
source("sixseven_data_aggregation.R")
source("wilcoxon_corrs_sixseven.R")
source("sixseven_figure_code.R")
source("sixseven_simplestats.R")
source("sixseven_simplegraphs.R")
source("sixseven_visit_ages_table.R")
source("sixseven_measures_table.R")
source("sixseven_vboost_table.R")
source("sixseven_reliability.R")


getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

#to do:
# * add author contribution note? something like this?
  #EB designed the research, analyzed the data, and drafted the paper. 
  #SD, SK, and ST collected and annotated/oversaw annotation of the data. 
  #AA wrote code-base for data annotation pipeline. AA, SD, and SK contributed to data aggregation and manuscript code. 
  #All authors contributed to writing. 

```
Researchers have studied development by observing infants experiencing their natural habitats for decades [@taine1876note; @williams1937analytical]. Over the past 20--30 years, written records have been increasingly supplemented with audio- and video-recordings, depicting infants' linguistic, social, and physical landscape. Such data --- often shared through repositories like CHILDES and Databrary --- in turn provide a proxy for various 'input' measures in theories of social, motor, and particularly, *linguistic* development [@macwhinney2001emergentist]. 

Furthermore, recent technological advances allow the collection of longer, denser, and higher-quality recordings, used to study infants' input and language skills [@bergelson2017nature; @roy2015predicting; @oller2010automated; @vandam2016homebank; @weisleder2013talking, *inter alia*]. Such naturalistic data seeks to reveal what infants actually learn from as they make use of their biological endowments and environmental resources.
 
While cutting-edge technologies make collecting observational data ever easier, this growing toolbox increases researchers' decision load, with serious but underexplored side-effects. Researchers must decide on recording modalities (e.g. audio, video, both), where, whom, and how long to record, and whether to capture structured or free-ranging interactions, with or without experimenters present. While any path through such decision-trees may lead to equivalent results, this is rarely tested. Problematically, this leads to research with theoretical conclusions built on unmeasured equivalency assumptions.
 
In recent work directly comparing sampling methods, @tamis2017power analyzed mother-infant behavior in 5-minute structured interactions and 45 minutes of free play at home. They found while language quantity across contexts correlated,  relative to free play, infants experienced more words per minute (both types and tokens) in structured interactions. They conclude that sampling must be matched with research question, cautioning that while brief samples may be appropriate for studying individual differences, extrapolations from short samples must be made with care. 

In contrast, work by @hart1995meaningful extrapolated extensively. Based on 30 hours of data per family (collected one hour per month for 2.5 years), these researchers estimated that by age four, children receiving public assistance (n=6) heard >30 million fewer words than professional-class children (n=13). While their results highlighting SES differences certainly merited (and received) follow-up [e.g. @fernald2013ses; @noble2005neurocognitive, *inter alia*], they have also been criticized as an extreme over-extrapolation [@dudley2009pathologizing; @michaels2013commentary].

Still other research analyzes base rates of certain linguistic phenomena to provide in-principle proof of what young children hear [@brent2001role; @tomasello2000young; @lidz2003infants]. Unfortunately, it is difficult to predetermine an 'appropriate' sample for such base rates. For instance, practically any length of adult speech, across wide-ranging recording parameters, will find function words (e.g. 'of') at much higher rates than content words (e.g. 'fork'). For questions concerning many aspects of infants' language input, however, it is largely unknown how sampling may bias results, and thus in practice, various practical constraints on data collection and availability take precedent.

We explore sampling directly, comparing hour-long video-recordings and daylong audio-recordings in a single sample of 44 infants, as part of a larger study on early noun learning. We annotated concrete nouns said to (or loudly and clearly near) infants. We further annotated three properties previously linked with early language learning: (1) utterance-type, which provides syntactic/situational information [@hoff2002children; @brent2001role; @debaryshe1993joint] (2) object presence (i.e. referential transparency), which clarifies whether spoken words' referents are visually appreciable [@bergelson2017nature; @bergelson2013acquisition; @yurovsky2013statistical; @cartmill2013quality], and (3) talker, which measures speaker quantity and prevalence [@rost2010finding; @bergmann2016discriminability].

This design sets up two overarching questions. First, does noun input in one video-recorded hour predict noun input in an entire audio-recorded day? Second, do input quantities differ once time is normalized? If the noun input is equivalent and predictive across recording-types, then researchers can freely vary observational data collection approaches with impunity. If not, understanding methodological biases is critical to ensuring that learning theories consider the data quantity and variability available to learners day-to-day.  

Here we compare language input across four key properties (word quantity/diversity,  utterance-type, object presence, and talker), as measured by hour-long videos and (separate) full-day audio-recordings. This seemingly methodological research has deep implications for developmental theory: we examine how sampling may alter conclusions about the linguistic input driving early development.

# Methods
## Participants
Infants were recruited from a database of families from local hospitals, or through BabyLab outreach. Forty-six participants enrolled; two dropped out leaving 44 in the final sample. All were full-term (40Â±3 weeks), had no known vision or hearing problems, and heard $\geq 75\%$ spoken English. Participants were 95% white; 75% of mothers had a B.A. or higher. The families were enrolled in a yearlong study that included monthly audio- and video-recordings, as well as in-lab visits every other month. See Table \@ref(tab:recording-ages-table) for age details. Here we report on the home recording data from the first two timepoints (6 and 7 months) of this study, for which participants were compensated $10.\footnote{We include only these timepoints because no infants had begun producing words themselves (which changes the input). Given the broader project aims, these timepoints alone had the entire daylong audio-recording annotated.}  

## Procedures
Participants gave consent at an initial lab visit for the larger study through a process approved by university IRB. Questionnaires concerning family/infant background, not germane to the present analysis, are reported elsewhere [@bergelson2017nature; @Laing_Bergelson_17]. Four recordings are analyzed for each infant: an audio- and video-recording at 6 and at 7 months, each on a different day\footnote{A video is missing for one infant due to technical error.}. See Table \@ref(tab:recording-ages-table). Through release forms collected after each month's recordings were complete, parents could approve data-sharing other authorized researchers. The released recordings are available via Databrary. 

## Video-Recordings
Researchers visited infants' homes each month to video-record an (ostensibly) typical hour of infants' lives. Infants were outfitted with a hat or headband affixed with two small Looxcie cameras (22g each). One camera was oriented slightly down and the other slightly up, to best capture infant's visual field (verified via Bluetooth with an iPad/iPhone during setup). A standard camcorder (Panasonic HC-V100 or Sony HDR-CX240) was positioned to best observe the infant, which parents were asked to move if they changed rooms. After set-up, experimenters left for one hour.

## Audio-Recordings
Audio-recordings captured up to 16 hours of infants' language input. Parents were given LENAs (LENA Foundation, Boulder, CO), small audio-recorders (<60g) along with infant vests with a LENA-sized chest pocket. Parents were asked to put the vest and recorder on babies from when they awoke to when they went to bed (excepting naps and baths). Parents were permitted to pause the recorder anytime but were asked to minimize such pauses.

```{r lengths, echo = F, warning = F, message = F}
vidmode <- getmode(round(vidtime$total_min)) #rounding to the minute 1st
audmode <- getmode(audtime$total_min)
audmode_nosil_hr <- round(getmode(audtime$tot_nosil/60),1)
aud_nosilM <- round(mean(audtime$tot_nosil)/60,2)
aud_nosilSD <- round(sd(audtime$tot_nosil)/60,2)
aud_nosilmin <- round(min(audtime$tot_nosil)/60,2)
aud_nosilmax <- round(max(audtime$tot_nosil)/60,2)

vidM_SD_R <- paste(round(mean(vidtime$total_min),2), "min., SD=",round(sd(vidtime$total_min),2),
                   ", R=",min(vidtime$total_min), "--", max(vidtime$total_min),"min.", sep = "")
audM_SD_R <- paste(round(mean(audtime$total_hr),2), "min., SD=",round(sd(audtime$total_hr),2),
                   ", R=",min(audtime$total_hr), "--", max(audtime$total_hr)," hr", sep = "")
audM_SD_Rmin <- paste(round(mean(audtime$total_min),2), "min., SD=",round(sd(audtime$total_min),2),
                   ", R=",min(audtime$total_min), "--", max(audtime$total_min),"min.", sep = "")

audnosilM_SD_R <- paste(aud_nosilM*60, "min., SD=",aud_nosilSD*60,", R=",aud_nosilmin*60, "--", aud_nosilmax*60, "min.", sep = "")
```

## Data Processing
Details of the entire data-processing pipeline are on OSF (https://osf.io/cxwyz/wiki/home/). Videos were processed using Vegas and in-house video-editing scripts. Footage was aligned in a single, multi-camera view before manual language annotation in Datavyu. Audio-recordings were initially processed by LENA proprietary software, which segments and diarizes each audio file; this output was then converted to CLAN format [@macwhinney2010transcribing]. Through in-house scripts, long periods of silence were marked in these CLAN files (e.g. when the audio vest was removed or during naps), which were then used for manual language annotation. 

Modally, videos were an hour (`r vidmode`min., *M*=`r vidM_SD_R`), and audio-recordings were 16 hours (`r audmode`min., *M*=`r audM_SD_Rmin`), the maximum capacity of the LENA. Removing the long silences from audio-recording left ~10hrs. of audio (Mode=`r audmode_nosil_hr*60`min., *M*=`r audnosilM_SD_R`). This comported with established norms for 6--8-month-olds in the US [@mindell2010cross]: 180min. of daytime sleep, and 600min. of nighttime sleep. All infants were awake the entire video-recording hour except one, whose video annotation ended at sleep onset). 

## Language Annotation
Trained researchers annotated each recording. This entailed demarcating each concrete noun directed to or easily overheard by the child (e.g. words directed at adjacent siblings), but not distant language (e.g. background television).'Object words' were operationalized as concrete, imageable nouns (e.g. shoe, arm). Each annotation noted the word and lemma (e.g. teethies, tooth), along with utterance-type, object presence, and talker. Utterance-type classified each object word utterance as declarative, question, imperative, reading, singing, short-phrase, or unclear. Short-phrase utterances included words in isolation and <3 word noun-phrases (e.g. 'the red ball' or 'kitty's paw'). Object-presence coded whether objects were present and attended to (yes/no). Lastly, talker included live interlocutors and electronics: mother, toy, etc.; this was checked by staff highly familiar with each family. We assessed intercoder reliability on a random contiguous 10% of the annotations in each file for the two categorical variables (utterance-type and object-presence). Reliability was moderate to strong (utterance-type: `r meanagreeUT`% agreement, Cohen's $\kappa$=`r meankappaUT`; object-presence: `r meanagreeOP`% agreement, Cohen's $\kappa$=`r meankappaOP`).

# Results
## Analysis Plan
Based on the coding scheme above, we derived 12 measures from each recordings' annotations for each child (n=44), recording-type (audio, video), and month (6, 7). See Table \@ref(tab:measures-tab). We further averaged across months to increase precision; furthermore, we have no theoretically-motivated reason to predict input differences across months (i.e. no developmental or linguistic milestones are typically achieved at 6--7 months.) While we initially anticipated analyzing the data with multi-level models, nearly all such models revealed highly skewed residuals (by Shapiro-Wilk Test), even when log-transformed, limiting interpretation across measures. Thus, we instead report a simple set of nonparametric analyses below. We used R for all analyses; the code that rendered this manuscript is on Github, to be shared upon publication.\footnote{Please contact corresponding author for access before publication.}

For all recording-type comparisons, we look at whether our measures *differed* significantly (by two-tailed, paired Wilcoxon Test) and *correlated* significantly (by Kendall Rank Correlation) across the given groups. This approach lets us compare, e.g., whether the time-normalized count of declarative nouns is indistinguishable in our audio- and video-recordings, independently of whether these values are correlated. We applied Holm's *p*-value adjustment for multiple comparisons [@holm1979simple], for the set of 12 Wilcoxon tests and the set of 12 Kendall Correlations. 

```{r scaling examination, echo = F, warning = F, message = F}
mean_vboost_measures <- vboost_mean_collapsed %>% 
  summarise(mean_measureboost = mean(vboost_types:vboost_op)) %>% as.double()
```

## Count Measure Analysis
To examine how the hour-long video data 'scale' to day-length data descriptively, we first divided the 12 count measures from the videos by those from the audio-recordings for each child, to derive 'video-fraction' scores (video/audio). This showed that the video-recordings were `r vboost_mean_collapsed$vboost_min` of the length of audio-recordings, or `r vboost_mean_collapsed$vboost_awakemin` of the length when audio-recording silences were removed. However, rather than a concomitant 10-fold decrease in our count measures (as would be expected if videos captured a 'representative' hour of the day), the fractions averaged to `r mean_vboost_measures`; see Table \@ref(tab:normtable). Thus, by and large, videos had a denser concentration of nouns across measures than did the audio-recordings. See Figure \@ref(fig:gr-derived-counts-67-diff) for raw count data for each metric. 

We computed video-fractions (rather than the reciprocal, i.e. audio/video) because there were more zero values for videos than audio-recordings (e.g. instances when children heard no sung nouns), rendering more undefined values. Indeed, >1/3 of children did not hear nouns in reading or from fathers on videos in either month. See Table \@ref(tab:propna-missing-tables). 

We next normed our counts by the number of minutes in each. E.g., if an infant heard 500 noun-tokens in 800 minutes of non-silent audio-recording, and 200 in 60 minutes of videos, this was normed to .62 and 3.3 noun-tokens per minute, respectively; zero values were retained within normed counts.\footnote{One infant's zero value was excluded from 'father' measures; this infant had no father at home.}

```{r wilcoxon-corr-stats, warning = F, echo = F, message = F}
w_sigps <- nrow(ws %>% filter(pval_adj<.05))
c_sigps <- nrow(cs %>% filter(pval_adj<.05))
tau_min <- round(min(c_taus_sig$estimate),2)
tau_max <- round(max(c_taus_sig$estimate),2)
tau_mean <- round(mean(c_taus_sig$estimate),2)
```

With the normed data, `r w_sigps`/12 metrics occurred at significantly lower rates in audio-recordings than video-recordings (all adjusted-*p*<.05). The remaining metric, nouns from fathers, was statistically indistinguishable across recording types (adjusted-*p*>.05). Thus, overall, per unit time, infants heard less noun input across our metrics of quantity, talker, utterance-type and object presence in audio-recordings than in videos (see Figure \@ref(fig:gr-derived-counts-67-diff) and \@ref(fig:gr-derived-normcounts-diff) for raw and normed count data).

Looking next at correlations, we find that `r c_sigps`/12 metrics correlated in audio vs. video data; nouns per minute heard from fathers and in singing did not. The size of the correlations (i.e. Kendall's $\tau$) was moderate (excluding the two non-significant metrics, *M*=`r tau_mean`, `r tau_min`-`r tau_max`, all adjusted-*p*<.05). See Table \@ref(tab:normtable) and Figure \@ref(fig:gr-derived-normcounts-corr). 

```{r prop-ut, echo = F, warning = F, message = F}
dq_propmeans <- sixseven_spreadAV_collapsed %>% 
  dplyr::select(a_propd, v_propd, a_propq, v_propq) %>% 
  summarise_all(.funs = mean)
#pvalues for d&q here:
#propws
```  

## Exploratory Analyses
Lastly, we undertook two sets of highly exploratory analyses, at the utterance and word level. The utterance-type analysis is based on the unanticipated observation that while declaratives and questions made up >2/3 of the input for each recording-type, the videos appeared to contain relatively more questions and fewer declaratives (See Fig \@ref(fig:gr-derived-counts-67-diff) and Fig \@ref(fig:gr-derived-normcounts-diff)). To test this statistically, we converted the six utterance-type counts to proportions (e.g. number of nouns heard in declaratives over total noun tokens) for each recording-type. Wilcoxon tests of each utterance-type in audio- vs. video-recording (corrected for multiple comparisons) revealed that indeed, declaratives and questions occurred at different rates in audio- and video-recordings (both adjusted-*p*<.05), with audio-recordings containing relatively fewer questions ($M_{video}$=`r dq_propmeans$v_propq`, $M_{audio}$=`r dq_propmeans$a_propq`) and more declaratives than videos ($M_{video}$=`r dq_propmeans$v_propd`, $M_{audio}$=`r dq_propmeans$a_propd`). No other proportional utterance-type differences reached significance across recording-types (all adjusted-*p*>.05). See Figure \@ref(fig:gr-ut-count-collapsed). 

At the word level, we aimed to characterize whether audio- and video-recordings captured the same nouns and the same relative frequencies across words and families. Nouns' frequency distribution was Zipfian: of the `r totaln_objectwords` unique object words (`r totaln_bl` lemmas) heard across months and recording-types, only `r totaln_once_objectwords` (`r totaln_once_bl` lemmas) occurred more than once.

```{r top-words, warning=F, message= F, echo = F}
avtop100_numwords <- top100av_spread %>% nrow()
avtop100corr_est_collapsed<- round(cor_AVtop_collapsed$estimate,2)
avtop100corr_pval_collapsed<- ifelse(cor_AVtop_collapsed$p.value<.0001, "*p*<.0001", "fix-this")
avtop100_numwordsnozeros <- top100av_spread_nozeros %>% nrow()
avtop100corr_estnozeros <- round(cor_AVtopnozeroes$estimate,2)
avtop100corr_pvalnozeros<- ifelse(cor_AVtopnozeroes$p.value<.0001, "*p*<.0001", "fix-this")

audio_nfams <- overall_top10_nfams %>% filter(audio_video=="audio")
video_nfams <- overall_top10_nfams %>% filter(audio_video=="video")

```
 
We examined the 100 most frequent nouns from audio- and video-recordings (n=`r avtop100_numwords` due to ties, n=`r avtop100_numwordsnozeros` without words that occurred zero times in one recording-type). Frequency across recording-types correlated significantly (Kendall's tau: `r avtop100corr_estnozeros`, `r avtop100corr_pvalnozeros`) even with zero-frequency words included (Kendall's tau: `r avtop100corr_est_collapsed`, `r avtop100corr_pval_collapsed`; see Figure \@ref(fig:top-100-logspace) and \@ref(fig:top100-corr-rectype) ). 

Finally, we analyzed the top ten words by recording-type. Four of the top ten words in each recording-type overlapped (baby, book, mouth, toes), suggesting that extremely common words are relatively conserved across recording-types. However, the top audio words were far more common across families (see Figure \@ref(fig:top10noun-freq)). Indeed, the ten most frequent nouns in audio-recordings were heard by `r audio_nfams$min_nfams`-`r audio_nfams$max_nfams` (*M*=`r audio_nfams$mean_nfams`(`r audio_nfams$sd_nfams`)) of the 44 families; those in video-recordings were heard by `r video_nfams$min_nfams`-`r video_nfams$max_nfams` (*M*=`r video_nfams$mean_nfams`(`r video_nfams$sd_nfams`)). Finally, the top audio words were ~3x as common as the top video words ($M_{audio}$=`r audio_nfams$mean_freq`(`r audio_nfams$sd_freq`), $M_{video}$=`r video_nfams$mean_freq`(`r video_nfams$sd_freq`)), again underscoring the higher density of nouns in video-recordings (which were ~1/10 the length of audio-recordings). Taken together, this exploratory analysis suggests that daylong audio-recordings may render more stable estimates of pervasively common words across families than do video-recordings. 
 
#Discussion
Our results can be distilled to three key findings. First, infants heard relatively more nouns in videos than in the audio-recordings. Per minute, infants heard ~2--4x more noun input across quantity, speaker, utterance-type, and object-presence metrics when video-recorded for an hour versus audio-recorded for a day. Second, while our metrics generally correlated across audio- and video-recordings, the relative rates of the most prevalent utterance-types and the rates of unattested data for certain metrics varied across them. Finally, while the highest frequency words across recording types largely overlapped and correlated, top words from the daylong audio-recording appear to better represent the noun input across families.

## Noun Quantity and Lexical Diversity
Overall, the pattern across recording-types primarily suggests a difference in volubility, since by-and-large measures both correlated and differed across recording-type. As Suskind et al.[-@suskind2013exploratory] noted about an intervention, daylong audio recordings likely provide more realistic counterparts to 'best behavior' hourlong video-sessions. We add that shorter video-recording itself may elicit higher speech-volume, separate from caretakers' deliberate intent.

Indeed, families likely found it simply easier to behave freely with infants in special vests than with cameras on their heads. *Apropos* equipment prominence, both âhatâ and âcameraâ were top-10 video words; no analogous nouns (e.g. vest, recorder) topped the frequency rankings in audio-recordings (see Figures \@ref(fig:top-100-logspace) and \@ref(fig:top10noun-freq)).

Our comparison across recording-types highlighted many differences across measures, even with family and age held constant. The quantity metrics provide a conceptual replication and extension of @tamis2017power. Despite numerous methodological differences (length/type of recordings compared, experimenter presence, age, word-class analyzed), both studies found that parent talk per unit time was significantly higher in shorter recordings. While the difference they find is less extreme numerically (roughly 1.5--2x the number of types and tokens in the longer vs. shorter recording compared to our 2--3x difference), this general pattern appears robust across our sampling methods. Taken together, this suggests that shorter recordings elicit denser caregiver talk.

For certain research questions, these differences in volubility and lexical diversity may not matter. E.g. for studies examining relative rates of word use and object interactions during a concentrated in-lab exposure and test phase, denser talk in shorter recordings may be less relevant. In contrast, research quantifying language input across populations with varying demographic, social, and cultural properties may need to be particularly sensitive to cross-sample comparison [cf. @bergelsonunderreview; @cristia2017child; @shneidman2012language].

##Object Presence
Object presence was higher in videos than in audio-recordings. This may be because the video-recordings truly had more object presence (i.e. infants mostly stayed in 1--2 rooms, interacting with what was at hand). Alternatively, it may be the case that there are more ambiguous cases of âobject presenceâ in audio-recordings than video-recordings, which may have contributed to ânot presentâ annotations at higher rates. Indeed, although object presence did correlate significantly across recording-types (`r subset(c_taus_sig,comp=="c_yop")$estimate`), inter-rater reliability was higher for videos than audio-recordings for this measure (audio: `r agreeaudOP$value`% agreement, Cohen's $\kappa$=`r kappaaudOP$value`; video: `r agreevidOP$value`% agreement, Cohen's $\kappa$=`r kappavidOP$value`). Our interpretation is that both factors are likely at play, i.e. that the object-presence difference we find reflects a true difference between situations that arise during daylong audio vs. hour-long video-recordings, and that there is more noise in the estimate of object-presence when visual information is unavailable. Given that object presence and the related ideas of referential transparency and contingent talk have been linked with early language development [@bergelson2017nature; @yurovsky2013statistical; @mcgillion2017randomised; @cartmill2013quality], this property merits follow-up. A better understanding of  situations and contexts that elicit contingent, referentially-transparent caretaker talk (around objects or otherwise) may be a fruitful avenue for further work.

##Talker Variability  
Infants heard nouns from more talkers per minute in videos than in audio-recordings, though in raw numbers infants heard roughly double the speakers over the course of a day as they heard in one video-recorded hour (see Figure \@ref(fig:gr-derived-counts-67-diff)). While we considered noun input from all sources (human, electronic, etc.), the quantity of talkers was largely swamped by input from mothers (~65%), which was also greater in videos than audio-recordings. The proportion of input from fathers did not vary by recording-type, though over half of videos did not include noun input from fathers at all. This is largely due to sample demographics: video-recording took place during weekday business hours, when the fathers in this sample were largely at work. In contrast, audio-recordings spanned work-hours and days of the week. Given that fathers and mothers make different contributions to early language development [@pancsofar2006mother], this is a clear example of a consequence of methodological choices: to better understand parents' input, considering work-schedules is critical.

Talker variability is also relevant for recent in-lab studies: while infants at the same age tested here looked equivalently to named images when words were produced by a new person or their mother [@bergelson2017young], slightly older infants showed a word-learning advantage when multiple talkers name new objects [@rost2010finding]. Furthermore, talker variability differentally influences certain phonetic discriminations [@bergmann2016discriminability]. Indeed, viable models of phonetic learning must encorporate mechanisms specifying the role of talker variability, even if only to show that a large range of talker input converges on effectively equivalent phonetic categories. Better approximations of infants' quotidian experiences are a prerequisite for such models. The present results indicate that such estimates of talker variability are inflated in hour-long videos relative to daylong audio-recordings.  

## Utterance-Types  
We found more nouns in every utterance-type in videos than in audio-recordings, per unit time. These utterance-types were a mix of largely syntactic constructions (declaratives, questions, imperatives, short phrases) and more situationally-defined utterance-types (reading, singing). While its not particularly surprising that reading or singing rates might vary across recording-types, we did not anticipate differences in declaratives and questions, which made up most of the input. Indeed, while questions and declaratives made up the majority of the input for each recording-type, videos had relatively more questions and fewer declaratives. This is key instance where methodological choices may influence language acquisition theories: base-rates of questions taken from videos would inflate estimates of auxiliary verbs in the early input. Notably, previous work has found that studies vary in whether they find links between questions (yes/no and wh-) in the input and childrenâs early productions, invoking developmental level to explain cross-study differences [@barnes1983characteristics; cf. @huttenlocher2002language]. We add the possibility that recording-type too may contribute to the base-rates of questions in the input, even with age kept constant.


## Top Words
Our interpretation of these results is that relatively short video-recordings overestimate young infantsâ typical noun input, and that extrapolation based on daylong audio-recordings likely better represents infantsâ daily lives. This underscores our third main finding: the conclusions one would draw about which words are most common in young infantsâ language input differ in their robustness across families by recording-type. That is, the top audio words were all heard by $\geq75\%$ of these families; only one of the top 10 video words ('hat') was this common, and was clearly tied to the recording equipment (see Figure \@ref(fig:top10noun-freq)). This result may be meaningful in several ways. First, corpora of child language input offer our best proxies for what infants learn from: our 'top words' analysis suggests that the input would seem far more heterogeneous across children based on hour-long video-recordings alone. Second, word frequency and prevalence across families are often used to select stimuli for in-lab study; relying on estimates from shorter, less representative recordings may stymie the words studied in the lab. Thus, understanding how cross-family noun-input stability scales with recording-length may prove critical for future research; the word-level results above are an initial exploration in understanding this dimension of naturalistic observational data.

## Limitations and Conclusions
Given the technical limitation that currently available small video-recorders have a shorter battery-life than audio-recorders, we cannot conclusively separate the effects of modality and length. That is, had we only audio-recorded for an hour or recorded video all day, we may have obtained equivalent results across recording modalities. Indeed, sub-sampling a single hour from each audio-recording would not be an appropriate comparison to the videos, given that this hour would either be constrained by time of day in a way the videos were not, or if random, may contain features wholly different from the video recordings, e.g. naps or travel in the car, and the absence of researchers and equipment bookending the session. More direct comparisons awaits technological progress. A further limitation is self-selection into the study: many parents are unwilling to invite researchers to record their infants. Relatedly, our convenience sample does not reflect the broader demographics of the US (let alone other cultures), and as such, should be extended to other populations before conclusive generalizations about sampling methodology can be made [cf. @bergelsonunderreview].

Understanding what infants learn from is a key part of understanding what and how they learn at all. Here we have taken first steps in understanding how two different data collection approaches may influence our conclusions about early linguistic input. We find that even naturalistic observer-free video-recordings appear to inflate language input relative to daylong recordings, in ways that influence syntactic constructions, word-specific experiences, talker-variability, and the sheer quantity and diversity of nouns infants hear. Work from the preceding decades suggests all of these factors matter for early learning. Yet without knowing how sampling methods may hamper us in principle, we necessarily limit our ability to adequately model infant language acquisition. The present work charts datapoints within this largely underspecified space, probing how robust linguistically-relevant measures are across naturalistic sampling methods of infants' everyday experiences.