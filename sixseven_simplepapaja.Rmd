---
title             : "Day by Day, Hour by Hour: Naturalistic Language Input to Infants"
shorttitle        : "Day by Day, Hour by Hour"

author: 
  - name          : "Elika Bergelson"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "417 Chapel Drive, Box 90086"
    email         : "elika.bergelson@duke.edu"
  - name          : "Andrei Amatuni"
    affiliation   : "1,2"
  - name          : "Shannon Dailey"
    affiliation   : "1,2"
  - name          : "Sharath Koorathota"
    affiliation   : "2,3"
  - name          : "Shaelise Tor"
    affiliation   : "2,4"
    
affiliation:
  - id            : "1"
    institution   : "Duke University"
  - id            : "2"
    institution   : "University of Rochester"
  - id            : "3"
    institution   : "Columbia University Medical Center"
  - id            : "4"
    institution   : "Syracuse University"

author_note: |
  Elika Bergelson, Psychology & Neuroscience, Center for Cognitive Neuroscience, Duke University;  Center for Developmental Science
  
  Andrei Amatuni, Psychology & Neuroscience, Duke University
  
  Shannon Dailey, Psychology & Neuroscience, Duke University
  
  Sharath Koorathota, Columbia University Medical Center
  
  Shaelise Tor, Marriage and Family Therapy, Syracuse University
  
  N.B.: All authors were in Brain & Cognitive Sciences at U. Rochester during data collection and have no COI to declare.

abstract: |
  Measurements of infants' quotidian experiences provide critical information about early development. However, the role of sampling methods in providing these measurements is rarely examined. Here we directly compare language input from hour-long video-recordings and daylong audio-recordings within the same group of 44 infants at 6 and 7 months. We compared 12 measures of language quantity and lexical diversity, talker variability, utterance-type, and object presence, finding moderate correlations across recording-types. However, video-recordings generally featured far denser noun input across these measures compared to the daylong audio-recordings, more akin to ‘peak’ audio hours (though not as high in talkers and word-types). Although audio-recordings captured ~10 times more awake-time than videos, the noun input in them was only 2--4 times greater. Notably, whether we compared videos to daylong audio-recordings or peak audio times, videos featured relatively fewer declaratives and more questions; furthermore, the most common video-recorded nouns were less consistent across families than the top audio-recording nouns were. Thus, hour-long videos and daylong audio-recordings revealed fairly divergent pictures of the language infants hear and learn from in their daily lives. We suggest short video-recordings provide a dense and somewhat different sample of infants’ language experiences, rather than a typical one, and should be used cautiously for extrapolation about common words, talkers, utterance-types, and contexts at larger timescales. If theories of language development are to be held accountable to 'facts on the ground' from observational data, greater care is needed to unpack the ramifications of sampling methods of early language input.
   
keywords          : "language acquisition, naturalistic observational data, infants, early home environment, language input, cognitive development"
wordcount         : "3979"

bibliography      : ["sixseven.bib", "r-references.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---

```{r read in data, comment=F, message=F, hide = T, warning = F, echo = F}
library("papaja")
library("knitr")
source("sixseven_data_aggregation.R")
source("same_top_combo.R")
#source("wilcoxon_corrs_sixseven.R")
#source("sixseven_figure_code.R")
source("sixseven_simplestats.R")
source("sixseven_simplegraphs.R")
source("sixseven_visit_ages_table.R")
source("sixseven_measures_table.R")
source("sixseven_vboost_table.R")
source("sixseven_reliability.R")


getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

#to do:
# * add author contribution note? something like this?
  #EB designed the research, analyzed the data, and drafted the paper. 
  #SD, SK, and ST collected and annotated/oversaw annotation of the data. 
  #AA wrote code-base for data annotation pipeline. AA, SD, and SK contributed to data aggregation and manuscript code. 
  #All authors contributed to writing. 

```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
```

# Highlights
  * We measured 44 infants’ early noun input during free-form interactions in hour-long videos and daylong audio-recordings; sampling approach shifted potential conclusions about home language environment.
  * Across quantity, utterance-type, object presence, and talker measures, nouns-per-minute were 2--4 times more frequent in video- than audio-recordings; videos were similar to *peak* audio hours.
  * Nouns in videos occurred relatively more often in questions and less often in declaratives than they did in daylong or peak audio-recording hours.
  * The most frequent nouns in daylong and peak audio-recording hours highly overlapped in identity and across families; this was less true for top video nouns.

Researchers have long studied development by observing infants in their natural habitats [@taine1876note; @williams1937analytical]. Over the preceding decades, written records have been increasingly supplemented with audio- and video-recordings, depicting infants' linguistic, social, and physical landscape. Such data --- often shared through repositories like CHILDES and Databrary --- provide a proxy for various 'input' measures in theories of social, motor, and particularly, *linguistic* development [@macwhinney2001emergentist]. 

Furthermore, recent technological advances have harnessed longer and denser recordings to study infants' input and language skills [@bergelson2017nature; @roy2015predicting; @oller2010automated; @vandam2016homebank; @weisleder2013talking, *inter alia*]. Such naturalistic approaches aim to characterize children's actual learning environment to better understand language acquisition.
 
However, wider-ranging technology creates more decision-points. Researchers must decide on recording modalities (e.g. audio, video), where, whom, and how long to record, and whether to capture structured or free-ranging interactions, with or without experimenters present. The equivalence of these decisions is rarely tested. Problematically, this leads to research with *theoretical* conclusions built on unmeasured *methodological* assumptions.

Recently, @tamis2017power directly compared sampling methods by analyzing mother-infant behavior in 5-minute structured interactions and 45min. of free play. They found that while language quantity across contexts correlated, infants experienced more words per minute in structured interactions than in free play. They conclude that sampling must match the research question, cautioning that extrapolations from short samples merit extra care. 

In contrast, work by @hart1995meaningful extrapolated extensively. Based on 30 recorded hours per family (collected over 2.5 years), they estimated that by age four, children receiving public assistance (*n*=6) heard >30 million fewer words than professional-class children (*n*=13). While their results merited and received follow-up [e.g. @fernald2013ses; @noble2005neurocognitive, *inter alia*], they have also been criticized as extreme over-extrapolation [@dudley2009pathologizing; @michaels2013commentary].

Still other research analyzes base rates of certain linguistic phenomena through child corpora [@brent2001role; @tomasello2000young; @lidz2003infants]. Unfortunately, predetermining 'appropriate' sampling for such base rates is difficult. For instance, practically any length of adult speech will find function words (e.g. 'of') at much higher rates than content words (e.g. 'fork'). For many questions, however, potential sampling bias is unknown, leaving practical constraints to guide sampling parameters.

We explore sampling directly, comparing hour-long video-recordings and daylong audio-recordings in a single sample of 44 infants, as part of a larger study on early noun learning. We annotated concrete nouns said to infants, focusing on nouns given their prevalence in early lexicons [@dale1996lexical]. We further annotated three properties previously linked with early language learning: utterance-type, which provides syntactic/situational information [@hoff2002children; @brent2001role; @debaryshe1993joint], object presence (i.e. referential transparency), which tags whether spoken words' referents are present and attended to [@bergelson2017nature; @bergelson2013acquisition; @yurovsky2013statistical; @cartmill2013quality], and talker, which measures talkers' quantity and prevalence [@rost2010finding; @bergmann2016discriminability].

This design sets up two overarching questions. First, does noun input in one video-recorded hour predict noun input in an audio-recorded day? Second, do input quantities differ once time is normalized? If the input is equivalent and predictive across recording-types, then observational data-collection approaches can vary with impunity. If not, understanding methodological biases can help learning theories incorporate appropriate bounds on data quantity and variability.

Thus, we examine home recordings across four key properties of language input: word quantity, utterance-type, object presence, and talker. This seemingly methodological investigation has deep implications for developmental theory: we examine how sampling may alter conclusions about the linguistic input driving early development.

# Methods
## Participants
Infants were recruited from a database of local families. Forty-six participants enrolled; two dropped out leaving 44 in the final sample. All were full-term (40±3 weeks), had normal vision and hearing, and heard $\geq 75\%$ spoken English. Participants were 95% white; 75% of mothers had $\geq$B.A. Families were enrolled in a yearlong study that included monthly audio- and video-recordings, as well as in-lab visits every other month. See Table \@ref(tab:recording-ages-table) for age details. Here we report on home recording data from the first two timepoints (6 and 7 months) of this study, for which participants received $10.\footnote{We used these timepoints because infants did not yet produce words themselves (which changes the input). Given the broader project aims, these timepoints alone had the entire daylong audio-recording annotated.}  

## Procedures
Participants gave consent at an initial lab visit for the larger study through a University IRB-approved process. Questionnaires concerning participant background, not germane here, are reported elsewhere [@bergelson2017nature; @Laing_Bergelson_17]. Four recordings are analyzed for each infant: an audio- and video-recording at 6 and 7 months, each on different days\footnote{One video is missing due to technical error.}. See Table \@ref(tab:recording-ages-table). Recordings that parents approved for sharing with researchers are on Databrary.

## Video- and Audio-Recordings
Researchers visited infants' homes each month to video-record a typical hour of infants' lives. Infants were outfitted with a hat or headband affixed with two small Looxcie cameras (22g each). One camera was oriented slightly down and the other slightly up, to best capture infant's visual field (verified via Bluetooth with an iPad/iPhone during setup). A standard camcorder (Panasonic HC-V100 or Sony HDR-CX240) on a tripod was positioned in the corner, which parents were asked to move if they changed rooms. After set-up, experimenters left for one hour.

Audio-recordings captured up to 16 hours of infants' input. Parents were given small audio-recorders (<60g) called LENAs (LENA Foundation, Boulder, CO), along with vests with LENA-sized chest pockets. Parents were asked to put the vest and recorder on babies from when they awoke to when they went to bed (excepting naps and baths). Parents were permitted to pause the recorder anytime but were asked to minimize such pauses.

```{r lengths, echo = F, warning = F, message = F}
vidmode <- getmode(round(vidtime$total_min)) #rounding to the minute 1st
audmode <- getmode(audtime$total_min)
audmode_nosil_hr <- round(getmode(audtime$tot_nosil/60),1)
aud_nosilM <- round(mean(audtime$tot_nosil)/60,2)
aud_nosilSD <- round(sd(audtime$tot_nosil)/60,2)
aud_nosilmin <- round(min(audtime$tot_nosil)/60,2)
aud_nosilmax <- round(max(audtime$tot_nosil)/60,2)

vidM_SD_R <- paste(round(mean(vidtime$total_min),2), "min., SD=",round(sd(vidtime$total_min),2),
                   ", R=",min(vidtime$total_min), "--", max(vidtime$total_min),"min.", sep = "")
audM_SD_R <- paste(round(mean(audtime$total_hr),2), "min., SD=",round(sd(audtime$total_hr),2),
                   ", R=",min(audtime$total_hr), "--", max(audtime$total_hr)," hr", sep = "")
audM_SD_Rmin <- paste(round(mean(audtime$total_min),2), "min., SD=",round(sd(audtime$total_min),2),
                   ", R=",min(audtime$total_min), "--", max(audtime$total_min),"min.", sep = "")

audnosilM_SD_R <- paste(aud_nosilM*60, "min., SD=",aud_nosilSD*60,", R=",aud_nosilmin*60, "--", aud_nosilmax*60, "min.", sep = "")
```

## Data Processing
Details of the entire data-processing pipeline are on OSF (https://osf.io/cxwyz/wiki/home/). Videos were processed using Vegas and in-house scripts. Footage was aligned in a single, multi-camera view before manual language annotation in Datavyu. Audio-recordings were initially processed by LENA proprietary software, which segments and diarizes each audio file; this output was then converted to CLAN format [@macwhinney2010transcribing]. After in-house scripts marked long periods of silence (e.g. naptimes), these files were used for manual language annotation. 

Modally, videos were an hour (`r vidmode`min., *M*=`r vidM_SD_R`), and audio-recordings were 16hrs. (`r audmode`min., *M*=`r audM_SD_Rmin`), LENA's maximum capacity. Removing the long silences from audio-recording left ~10hrs. of audio (Mode=`r audmode_nosil_hr*60`min., *M*=`r audnosilM_SD_R`), consistent with established wakeful daytime norms for 6--8-month-olds in the U.S. [@mindell2010cross]. All infants were awake for video-recording except one, whose video annotation ended at sleep onset. 

## Language Annotation
Trained researchers annotated each recording. This entailed demarcating each concrete noun directed to or said loudly and clearly near the child (e.g. at adjacent siblings), but not distant language (e.g. background television). 'Object words' were operationalized as concrete, imageable nouns (e.g. shoe, arm). Each annotation noted the noun and lemma (e.g. teethies, tooth), along with *utterance-type*, *object-presence*, and *talker*. *Utterance-type* classified each noun's utterance as declarative, question, imperative, reading, singing, short-phrase, or unclear. (Short-phrases included isolated words and <3-word noun-phrases, e.g. 'the red ball' or 'kitty's paw'.) *Object-presence* coded whether objects were present and attended to (yes/no) based on linguistic context (e.g. 'here's your spoon!' was coded 'yes'); in videos, visual context was also used. Lastly, *talker* tagged live interlocutors and electronics, checked by staff highly familiar with each family. We assessed intercoder reliability on a random contiguous 10% of annotations in each recording for the two categorical variables (utterance-type and object-presence). Reliability was moderate to strong (utterance-type: `r meanagreeUT`% agreement, Cohen's $\kappa$=`r meankappaUT`; object-presence: `r meanagreeOP`% agreement, Cohen's $\kappa$=`r meankappaOP`).

# Results
## Analysis Plan
Based on the coding scheme above, we derived 12 measures from each recording for each child (*n*=44), recording-type (audio, video), and month (6, 7). See Table \@ref(tab:measures-tab). We averaged across months to increase precision and because we lacked theoretically-motivated reasons to predict cross-month differences (i.e. no developmental or linguistic milestones are typically achieved at 6--7mo.) Unfortunately, multi-level models were not viable due to highly skewed residuals (by Shapiro-Wilk Test) even when log-transformed, limiting cross-measure interpretation. Instead we report a simple set of nonparametric analyses, conducted in R. The code that rendered this manuscript is on Github.\footnote{Please contact corresponding author for access before publication.}

For all recording-type comparisons, we look at whether our measures *differed* significantly (by two-tailed, paired Wilcoxon Test) and *correlated* significantly (by Kendall Rank Correlation) across the given groups. This approach lets us compare, e.g., whether time-normalized counts of declarative nouns are indistinguishable in audio- and video-recordings, independently of whether they are correlated. We applied Holm's *p*-value adjustment for multiple comparisons [@holm1979simple] for each set of Wilcoxon tests and Kendall Correlations.

```{r scaling examination, echo = F, warning = F, message = F}
mean_vboost_measures <- vboost_mean_collapsed %>% 
  summarise(mean_measureboost = mean(vboost_types:vboost_op)) %>% as.double()
```

## Count Measure Analysis
To examine how the hour-long video data scale to day-length data descriptively, we first divided the 12 count measures from the videos by those from the audio-recordings for each child to derive 'video-fraction' scores (video/audio). We opted for video-fractions (rather than audio/video) to minimize undefined values (e.g. 34% of children heard no nouns in reading utterances in their video-recordings; see Table \@ref(tab:propna-missing-tables)). This analysis showed that the video-recordings were `r vboost_mean_collapsed$vboost_min` of the length of audio-recordings, or `r vboost_mean_collapsed$vboost_awakemin` with audio-recording silences removed. However, rather than a concomitant 10-fold decrease in the count measures (as would be expected if videos captured a representative hour), the fractions averaged to `r mean_vboost_measures`; see Table \@ref(tab:normtable). Thus, by and large, videos had a denser concentration of nouns across measures than audio-recordings did. See Figure \@ref(fig:gr-derived-counts-67-diff). 

We next normed our counts by recording minutes. For example, if an infant heard 500 noun-tokens in 800 non-silent audio-recording minutes and 200 in 60 video-minutes, this was normed to .62 and 3.3 noun-tokens/minute, respectively; zero values were retained.\footnote{One infant's data was excluded from 'father' measures; this infant had no father at home.}

```{r wilcoxon-corr-stats, warning = F, echo = F, message = F}
w_sigps <- nrow(ws %>% filter(pval_adj<.05))
c_sigps <- nrow(cs %>% filter(pval_adj<.05))
tau_min <- round(min(c_taus_sig$estimate),2)
tau_max <- round(max(c_taus_sig$estimate),2)
tau_mean <- round(mean(c_taus_sig$estimate),2)

nn_c_sigps <- nrow(nn_cs %>% filter(pval_adj<.05))
nn_tau_min <- round(min(nn_c_taus_sig$estimate),2)
nn_tau_max <- round(max(nn_c_taus_sig$estimate),2)
nn_tau_mean <- round(mean(nn_c_taus_sig$estimate),2)

s_t_overlap <- top_same_overlappers %>% filter(!SubjectNumber %in% c("25_07","12_06","37_06")) %>% summarise(s_t_overlap=n_distinct(SubjectNumber))
SHw_sigps <- nrow(SHws %>% filter(pval_adj<.05))
THw_sigps <- nrow(THws %>% filter(pval_adj<.05))

```

Measuring correlations across recording types, we found that `r c_sigps`/12 metrics correlated in audio vs. video data; nouns per minute heard from fathers and in singing did not. Correlation magnitude (i.e. Kendall's $\tau$) was moderate (excluding the two non-significant metrics, *M*=`r tau_mean`, `r tau_min`-`r tau_max`, all adjusted-*p*<.05). See Table \@ref(tab:normtable) and Figure \@ref(fig:gr-derived-normcounts-corr).\footnote{The same pattern emerged with raw counts, except nouns from fathers also correlated significantly (adjusted-*p*<.05).}

We next compared the rates of each measure in three ways. First, we used the normed data, looking at counts per minute. With the normed data, `r w_sigps`/12 metrics occurred at significantly lower rates in audio-recordings than video-recordings (all adjusted-*p*<.05). The remaining metric, nouns from fathers, was statistically indistinguishable across recording types (adjusted-*p*>.05). Thus, overall, per unit time, infants heard less noun input in audio-recordings than in videos, across metrics of quantity, talker, utterance-type and object presence (see Figure \@ref(fig:gr-derived-counts-67-diff) and \@ref(fig:gr-derived-normcounts-diff)).

Next, we compared two different hour-long subsets from the daylong audio recording for comparison with the video-recorded hour, collectively referred to as 'peak' audio times. The *top* hour was the hour in which infants heard the most nouns. Complementarily, under the logic that parents scheduled video-recordings to optimize infant alertness, we extracted that *same* hour in the daylong audio; e.g., if the video recording visit was scheduled from 2:00pm--3:00pm, we used 2:00pm--3:00pm from that child's daylong audio recording that month. Our 12 measures were then computed in both the *top* and *same* audio hours. These hours only overlapped in `r s_t_overlap$s_t_overlap`/88 recordings (`r round(s_t_overlap$s_t_overlap/88,2)*100`%).\footnote{In 3 cases, the video-recording time (i.e. 'same' time) preceded the beginning of the daylong audio-recording (by 5, 30, or 90 minutes); in those cases the first hour of the recording was used. This created two further cases of 'top' and 'same' overlap.}

The results in video and same audio hours were indistinguishable for `r 12-SHw_sigps`/12 measures; the remaining `r SHw_sigps` occurred at significantly *higher* rates in the same audio hour (all adjusted-*p*<.05): number of speakers, noun types, nouns from fathers, and nouns in declaratives. Similarly, `r THw_sigps`/12 occurred at significantly higher rates in top audio hour than in videos (all adjusted-*p*<.05); these included those from the same audio comparison along with noun tokens, nouns in imperatives and nouns in short phrases. Taken together, the videos presented a somewhat different language input profile than the peak audio hours of the day: videos featured less input for some quantity, talker, and utterance-type measures, but were statistically indistinguishable in object presence, input from mothers, and input in other utterance-types. This same qualitative pattern held when looking at the rate of 'zero' values for the peak audio hours, relative to videos and daylong audio-recordings (see Table \@ref(tab:propna-missing-tables)).


```{r prop-ut, echo = F, warning = F, message = F}
dq_propmeans <- sixseven_spreadAV_collapsed %>% 
  dplyr::select(a_propd, v_propd, a_propq, v_propq) %>% 
  summarise_all(.funs = mean)
THdq_propmeans <- TOPHOURsixseven_spreadAV_collapsed %>% 
  dplyr::select(a_propd, a_propq) %>% 
  summarise_all(.funs = mean)
SHdq_propmeans <- SAMEHOURsixseven_spreadAV_collapsed %>% 
  dplyr::select(a_propd, a_propq) %>% 
  summarise_all(.funs = mean)
#pvalues for d&q here:
#propws%>% filter(pval_adj<.05)
#SHpropws %>% filter(pval_adj<.05)
#THpropws%>% filter(pval_adj<.05)
```  

## Exploratory Analyses
Lastly, we undertook two sets of highly exploratory analyses at the utterance and word levels. The first was based on the unanticipated result that while declaratives and questions made up >2/3 of the input for each recording-type, the videos appeared to contain relatively more questions and fewer declaratives (See Fig. \@ref(fig:gr-derived-counts-67-diff) and \@ref(fig:gr-derived-normcounts-diff)). To test this statistically, we converted the six utterance-type counts to proportions (e.g. declarative nouns/total nouns). Wilcoxon tests of each utterance-type in audio- vs. video-recording (corrected for multiple comparisons) revealed that indeed, declaratives and questions occurred at different rates across recording-types, with audio-recordings containing relatively fewer questions ($M_{video}$=`r dq_propmeans$v_propq`, $M_{audio}$=`r dq_propmeans$a_propq`, $M_{same\  audio}$=`r SHdq_propmeans$a_propq`, $M_{top\  audio}$=`r THdq_propmeans$a_propq`) and more declaratives than videos ($M_{video}$=`r dq_propmeans$v_propd`, $M_{audio}$=`r dq_propmeans$a_propd`, $M_{same\  audio}$=`r SHdq_propmeans$a_propd`, $M_{top\ audio}$=`r THdq_propmeans$a_propd`; each video vs. audio comparison adjusted-*p*<.05). No other proportional utterance-type differences reached significance across recording-types (all adjusted-*p*>.05). See Figure \@ref(fig:gr-ut-count-collapsed). 

At the word level, we aimed to characterize whether audio- and video-recordings captured similar nouns at similar relative frequencies across words and families. Nouns' frequency distribution was Zipfian: of the `r totaln_objectwords` unique object words (`r totaln_bl` lemmas), only `r totaln_once_objectwords` (`r totaln_once_bl` lemmas) occurred >1 time.

```{r top-words, warning=F, message= F, echo = F}
avtop100_numwords <- top100av_spread %>% nrow()
avtop100corr_est<- round(cor_AVtop$estimate,2)
avtop100corr_pval<- ifelse(cor_AVtop$p.value<.0001, "*p*<.0001", "fix-this")
avtop100_numwordsnozeros <- top100av_spread_nozeros %>% nrow()
avtop100corr_estnozeros <- round(cor_AVtopnozeroes$estimate,2)
avtop100corr_pvalnozeros<- ifelse(cor_AVtopnozeroes$p.value<.0001, "*p*<.0001", "fix-this")

# audio_nfams <- overall_top10_nfams %>% filter(audio_video=="audio")
# video_nfams <- overall_top10_nfams %>% filter(audio_video=="video")
audio_nfams <- timeslices_overall_top10_nfams %>% filter(timeslice=="audio_day")
video_nfams <- timeslices_overall_top10_nfams %>% filter(timeslice=="video_hr")
thaudio_nfams <- timeslices_overall_top10_nfams %>% filter(timeslice=="audio_tophr")
shaudio_nfams <- timeslices_overall_top10_nfams %>% filter(timeslice=="audio_samehr")

```
 
We examined the 100 most frequent nouns from audio- and video-recordings (*n*=`r avtop100_numwords` due to ties, *n*=`r avtop100_numwordsnozeros` excluding words that never occurred in one recording-type). Frequency across recording-types correlated significantly (Kendall's $\tau$: `r avtop100corr_estnozeros`, `r avtop100corr_pvalnozeros`) even with zero-frequency words included (Kendall's $\tau$: `r avtop100corr_est`, `r avtop100corr_pval`; see Figure \@ref(fig:top-100-logspace) and \@ref(fig:top100-corr-rectype)).\footnote{The same pattern held with video compared to peak audio hours.} 

Lastly, we analyzed the top ten nouns within videos, daylong-audios, and both peak audio hours. Four of the top ten words in each time sample overlapped (baby, book, mouth, toes), suggesting that extremely common nouns are relatively well-conserved. Moreover, all but one word in the top 10 was identical for all 3 audio-based time-slices, while 5 of the top video words were unique to video recordings (see Figure \@ref(fig:top10noun-freq)).

The top 10 words within each time sample also varied in how common they were across the 44 families: top words from daylong audio occurred in `r round(audio_nfams$mean_nfams/44,2)*100`% of families (*M*=`r audio_nfams$mean_nfams`(`r audio_nfams$sd_nfams`); those in video-recordings were heard by `r round(video_nfams$mean_nfams/44,2)*100`% (*M*=`r video_nfams$mean_nfams`(`r video_nfams$sd_nfams`)). Nouns in peak audio hours patterned in between (top hour: `r round(thaudio_nfams$mean_nfams/44,2)*100`% (*M*=`r thaudio_nfams$mean_nfams`(`r thaudio_nfams$sd_nfams`); same hour: `r round(shaudio_nfams$mean_nfams/44,2)*100`%, *M*=`r shaudio_nfams$mean_nfams`(`r shaudio_nfams$sd_nfams`)).

Finally, the top audio words were ~3 times as common as the top video words ($M_{audio}$=`r audio_nfams$mean_freq`(`r audio_nfams$sd_freq`), $M_{video}$=`r video_nfams$mean_freq`(`r video_nfams$sd_freq`)), further underscoring the higher density of nouns in video-recordings. Peak audio hour words were again between video and daylong audio ($M_{top\ audio}$=`r thaudio_nfams$mean_freq`(`r thaudio_nfams$sd_freq`); $M_{same\ audio}$=`r shaudio_nfams$mean_freq`(`r shaudio_nfams$sd_freq`)). Taken together, this exploratory analysis suggests that daylong audio-recordings may render more stable estimates of pervasively common words across families than do video-recordings. 
 
#Discussion
Our results can be distilled to three key findings. First, the density of noun input in hour-long video recordings was more similar to peak times in daylong audio recordings rather than to the day at large. Per minute, infants heard ~2--4x more noun input across quantity, speaker, utterance-type, and object-presence measures when video-recorded for an hour versus audio-recorded for a day. Second, while our metrics generally correlated across recording-types and many gross patterns were conserved across them, audio- and video-recordings differed in the relative rates of the top utterance types. That is, videos featured more questions and fewer declaratives than audio-recordings did. Finally, while the highest frequency words across recording types largely overlapped and correlated, top words from the daylong audio-recording appear to better represent the noun input across families.

Before delving into our findings more closely, we note that the bigger picture point here applies broadly: the way that we sample spontaneous behavior is influenced not only by an individual’s knowledge, feelings, and desires, but also by the context in which their behaviors are elicited. Indeed, this issue applies to other aspects of development, e.g. motor development and clinical behavior [@adolph2008shape; @gardner2000methodological] and permeates the social sciences: relevant findings from the adult economics, emotion, and social psychology literature also indicate that sampling approaches influence the volubility or range of human behavior [@coutinho2017effect;@ballard2008all; @altmann1974observational; @levitt2007laboratory].

## Noun Quantity and Lexical Diversity
Overall, the pattern across recording-types primarily suggests a difference in volubility, since by-and-large, measures both correlated and differed quantitatively by recording-type. As Suskind et al.[-@suskind2013exploratory] noted regarding interventions, daylong audio-recordings likely provide more realistic counterparts to 'best behavior' hour-long videos. We add that shorter video-recording itself may influence volubility, resulting in samples more akin to the high points in the natural ebb and flow over the day.

Indeed, families likely found it simply easier to behave freely with infants in special vests than with cameras on their heads. Our finding that both ‘hat’ and ‘camera’ were top-10 video words supports this idea; no analogous nouns (e.g. vest, recorder) topped the audio-recording frequency rankings (see Figures \@ref(fig:top-100-logspace) and \@ref(fig:top10noun-freq)). Anecdotally, while infants often required coaxing to wear the video-recording gear, no such issues emerged for audio-recording.   

Given that we held family and age constant, we expected many similarities across recording-types; nevertheless, differences also emerged. Indeed, the quantity metrics provide a conceptual replication and extension of @tamis2017power. Despite numerous methodological variations (recording types and lengths, experimenter presence, age, word-class), both studies found that parent talk per unit time was significantly higher in shorter recordings on average, but lower than the *highest* portion of the longer recordings. This general pattern appears robust across our sampling methods. Taken together, this suggests that shorter recordings elicit denser, though not maximal, caregiver talk compared to what infants typically experience.

For certain research questions, such quantity differences may not matter, e.g. for studies examining *relative* word rates or object interactions in concentrated in-lab exposures. In contrast, research quantifying language input across populations with varying demographic, social, and cultural properties may need to be particularly sensitive to cross-sample comparison [cf. @bergelsonunderreview; @cristia2017child; @shneidman2012language].

##Object Presence
Rates of object presence were higher in videos than daylong audio-recordings, but equivalent between videos and peak audio times. Given that object presence correlated across recording-types within children ($\tau$=`r subset(c_taus_sig,comp=="c_yop")$estimate`), our interpretation is that during higher talk volume times (i.e. video recordings and peak audio hours), nouns did occur with more object presence (i.e. infants mostly stayed in 1--2 rooms, interacting with what was at hand). However, since object presence was coded based on linguistic context and, when available, visual context, it's possible that indistinguishable object presence across video and peak audio is due to a combination of noise and systematic bias in coding object presence without visual context. Because object presence and the related ideas of referential transparency and contingent talk have been linked with early language development based on both audio-only and video-recordings [@bergelson2017nature; @yurovsky2013statistical; @mcgillion2017randomised; @cartmill2013quality], we find this latter possibility somewhat unlikely.
<!-- (audio: `r agreeaudOP$value`% agreement, Cohen's $\kappa$=`r kappaaudOP$value`; video: `r agreevidOP$value`% agreement, Cohen's $\kappa$=`r kappavidOP$value`) -->
Indeed, a better understanding of what elicits contingent, referentially-transparent caretaker talk may be a fruitful avenue for further work. 
```{r}
avg_prop_mom <- sixseven_basiclevel_home_data_agg %>%  summarise(mom_mean= mean(prop_mom))
```

##Talker Variability  
Infants heard nouns from more talkers per minute in videos than in daylong audio-recordings. In contrast, infants heard roughly double the speakers over the course of a day as they heard in one video-recorded hour and significantly more talkers during peak audio times than during videos (see Fig. \@ref(fig:gr-derived-counts-67-diff)). 

Notably, while we considered noun input from all sources, `r avg_prop_mom$mom_mean*100`% of infants' input came from mothers. Here, peak audio and video input from mothers was equivalent, though in comparison with daylong audio, there again were more nouns per minute from mothers in videos. In contrast, input from fathers was the only measure that did not vary in videos vs. daylong audio-recordings in the normalized count data. However, in the peak audio hours, there were more nouns from fathers than in the videos. Relatedly, >50% of videos captured *no* input from fathers. We believe this is because video-recording took place during weekday business hours when fathers in this sample were largely at work, while audio-recordings spanned work-hours and days. Given that fathers and mothers contribute differentially to early language development [@pancsofar2006mother], this is a clear example of a consequence of methodological choices. To better understand parents' input, considering work-schedules is critical. Put otherwise, home-recordings scheduled at the researcher and primary caretaker's convenience will likely undersample other caretakers.

The present results suggest that while infants hear most of their input from their mothers, they also hear several other speakers during high talk-volume times. Such data in turn feed infants' word-form representations. Indeed, recent lab studies have found that at the same age tested here, infants looked equivalently to named images when words were produced by a new person or their mother [@bergelson2017young], suggesting some degree of cross-talker normalization is in place around 6 months. In contrast, 14-month-olds' learning of similar-sounding words improves after training with tokens from multiple speakers [@rost2010finding], suggesting that even small amounts of talker variability aid new learning. This dovetails nicely with recent work showing that talker variability differentally influences certain phonetic discriminations [@bergmann2016discriminability]. While a wide range of talker and token distributionss surely result in appropriately language-specific phonetic categories, we suggest that learning models incorporating a large dose of input from a single talker alongside smaller doses of input from 3+ other talkers may help inform word-form knowledge in infants similar to those tested here.

## Utterance-Types  
Per unit time, we found more nouns in every utterance-type in videos than in daylong audio-recordings. In particular, we did not anticipate differences in declaratives and questions. Indeed, while these utterance-types universally made up the majority of noun input, videos had relatively more questions and fewer declaratives. This is a key instance where methodological choices may influence language acquisition theories: base rates of questions taken from videos would inflate estimates of auxiliary verbs in the input. Notably, previous work has varied in whether links between questions in the input and children’s early productions emerged, with developmental level invoked to explain cross-study differences [@barnes1983characteristics; cf. @huttenlocher2002language]. We add the possibility that recording-type too may contribute to the base rates of questions in the input, even with age and recording length kept constant.

## Top Words
Our third key finding concerned noun frequency and commonality across families. We found that top words in the daylong audio-recordings were heard by $\geq84\%$ of families; only 1/10 top video words ('hat') was this common, a clear vestige of our recording equipment (see Figure \@ref(fig:top10noun-freq)). This result may be meaningful in several ways. First, our analysis suggests that the input would seem far more heterogeneous across children based on hour-long video-recordings than it really is. Second, word frequency and prevalence are often used to select stimuli for in-lab study; relying on estimates from shorter, less representative recordings may stymie the words studied in the lab. Thus, understanding how cross-family noun-input stability scales with recording-length and type may prove critical for future research. The word-level results above are an initial exploration in understanding this dimension of naturalistic observational data.

## Limitations and Conclusions
Given the technical limitation on battery life for small video-recorders, we cannot conclusively separate the effects of modality and length. That is, had we recorded daylong videos, we may have obtained equivalent results across recording-types. Indeed, our peak audio analyses provided some evidence that videos are more akin to particularly language-saturated parts of infants' experience. However, the peak audio comparisons are imperfect since these hours were not bookended by researchers arriving and departing with pesky gear; further comparisons await technological progress. Importantly, we do not mean to suggest that audio reigns supreme: for many language-relevant questions concerning gaze, gesture, and visual perception, it is simply insufficient. We note too that while infants were willing to wear the LENAs and vests, there's a potential for caregiver compliance issues: parents may forget to start the recorder on wakeup or put it back on after naptime, accidentally pause or stop recordings, etc.   

A further limitation here is self-selection: many parents are unwilling to invite home recordings. Relatedly, our participants do not reflect U.S. demographics (let alone those elsewhere), and should be extended to other populations before conclusive generalizations about sampling methodology can be made [cf. @bergelsonunderreview].

Understanding what infants learn from is a key part of understanding what and how they learn at all. These are first steps in unpacking how two different data collection approaches may influence conclusions about early linguistic input, with a narrow focus on the initially dominant lexical class of nouns. We find that even naturalistic observer-free video-recordings appear to inflate language input relative to daylong recordings, in ways that influence syntactic constructions, word-specific experiences, talker-variability, and the sheer quantity and diversity of nouns infants hear. Work from the preceding decades suggests these factors matter for early learning. Yet without knowing how sampling methods may constrain results, we necessarily limit adequate models of language acquisition. The present work charts datapoints within this largely underspecified space, probing the robustness of linguistically-relevant measures across naturalistic sampling methods of infants' everyday experiences.\newpage
```{r create_r-references}
r_refs(file = "r-references.bib")
```

```{r recording-ages-table, comment=F, message=F, hide = T, warning = F, echo = F, results = "asis"}
apa_table(ages_table_data, caption = "Infant ages at home recordings and enrollment lab visit")
```
\pagebreak
```{r measures-tab, echo = F, warning = F, message = F, results = "asis"}
apa_table(measures_table_data2,caption = "Count measures (*n*=12), by Measure-Type",small = T)
```
\pagebreak
```{r propna-missing-tables, echo = F, warning = F, message = F, results = "asis"}
apa_table(all_vars_with_NAs, caption = "Proportion of infants with no recorded nouns for the listed measures, by sample", small = T, note = "V=video, A=daylong audio, Top=top hour of A, Same = Video-hour of A. All infants heard nouns for all other measures (see Table 2).")
```
\pagebreak
```{r normtable, echo = F, warning = F, results = "asis"}
apa_table(countvals_normed_vboost_table,cap="Video/Audio Count Measures, normed by minutes in recording (column 2) and divided without norming (column 3)", small = T, note = "If videos contained equivalent quantities of nouns, Inflation values would be 1, and Video-fractions would be .1")
```

```{r gr-derived-counts-67-diff, fig.cap = cap, echo = F, warning = F, fig.height=5, fig.width=7.5}
gr_countvals_long_collapsedNEW
cap <- "Noun count measures across audio-recordings and videos. Top row depicts daylong audio data; bottom row shows the 3 hour-long annotations: 'same' and 'top' are the two peak audio times, and 'video' indicates the video data. Upper panel labels indicate annotated sample length (day or hour); the bottom panel labels reflects measure type (op = object presence; utt = utterance-type, quant = quantity, Nspeakers = number of speakers). Bars (left to right) appear in legend order (top to bottom) in both color (count measures) and opacity (time sample: day, top-hour, same-hour, or video)."
```

```{r gr-derived-normcounts-diff, fig.cap = cap, echo = F, warning = F, fig.height = 5, fig.width =5.5}
gr_countvals_long_norm_collapsed
cap <- "Noun count measures normalized by recording length, for audio-recordings (solid borders) and videos (dashed borders). Normalized counts were calculated by dividing raw counts (see Fig 1.) by non-silent recording minutes. op = object presence; utt = utterance-type, quant = quantity, Nspeakers = number of speakers. Bars (left to right) appear in legend order (top to bottom). All measures differed significantly across recording-types except nouns from fathers."
```

```{r gr-derived-normcounts-corr, fig.cap = cap, echo = F,  fig.height= 3, warning = F}
gr_count_cor_VA_facetmonth_norm_collapsed
cap <- "Normalized count correlations between audio- vs. video-recordings. Each point indicates nouns per minute of recording for each child, averaged across months 6 and 7, for each measure. Point-shape indicates measure type. Robust linear correlations are plotted for visualization only; non-parametric correlations (Kendall) were computed for analysis, showing that all correlations were significant except nouns from fathers and in singing."
```

```{r gr-ut-count-collapsed, fig.cap = cap, echo = F,  fig.height = 3, fig.width = 4, warning = F}
gr_ut_count_collapsedNEW
cap <- "Utterance-type proportions across audio-recordings (daylong, 'same' hour and 'top' hour) and videos (indicated by line-type). Utterance-types are in legend order top to bottom. Videos contained a significantly more questions and fewer declaratives than the audio-recording time samples."
```

```{r top-100-logspace, fig.cap = cap, echo = F, warning = F, message= F, fig.width = 9, fig.height = 7}
top100_logspace_av_graph
cap <- "Log-scaled counts of the top 100 words in audio- and video-recordings. Each node represents the averaged count, across all participants in both months, of each noun (0.1 was added before taking logs to include 0 counts.) Words in blue occurred 0 times in one recording type; words in pink were attested in both recording types. Nodes are jittered for visual clarity, with grey lines indicating node location on axes."
```  

```{r top100-corr-rectype, fig.cap = cap, echo = F, fig.height = 3, fig.width = 3}
gr_top100_avspread_collapsed
cap <- "Correlations of the frequencies of the top 100 words in audio- vs. video-recordings. Each node represents one word averaged across all participants in both months."
```

```{r top10noun-freq, fig.cap = cap, echo = F, warning = F, fig.height = 3, fig.width = 8, fig.align="h"}
top10_graph_collapsedNEW
cap<-"Top 10 words by recording type and time sample. Each node represents the frequency count of each top audio or video word over both months (x-axis) and the number of families where that word was said (out of 44) across months (y-axis)."
```  

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
