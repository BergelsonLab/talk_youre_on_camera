---
title             : "Day by Day, Hour by Hour: Naturalistic Language Input to Infants"
shorttitle        : "Day by Day, Hour by Hour"

author: 
  - name          : "Elika Bergelson"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "417 Chapel Drive, Box 90086"
    email         : "elika.bergelson@duke.edu"
  - name          : "Andrei Amatuni"
    affiliation   : "1,2"
  - name          : "Shannon Dailey"
    affiliation   : "1,2"
  - name          : "Sharath Koorathota"
    affiliation   : "2,3"
  - name          : "Shaelise Tor"
    affiliation   : "2,4"
    
affiliation:
  - id            : "1"
    institution   : "Duke University"
  - id            : "2"
    institution   : "University of Rochester"
  - id            : "3"
    institution   : "Columbia University Medical Center"
  - id            : "4"
    institution   : "Syracuse University"

author_note: |
  Elika Bergelson, Psychology & Neuroscience, Center for Cognitive Neuroscience, Duke University;  Center for Developmental Science
  
  Andrei Amatuni, Psychology & Neuroscience, Duke University
  
  Shannon Dailey, Psychology & Neuroscience, Duke University
  
  Sharath Koorathota, Columbia University Medical Center
  
  Shaelise Tor, Marriage and Family Therapy, Syracuse University
  
  N.B.: all authors were in Brain & Cogsci at U. Rochester during data collection and have no COI to declare

abstract: |
  Measurements of infants' quotidian experiences provide critical information about early development. However, the role of sampling methods in providing these measurements is rarely examined. Here we directly compare language input from hour-long video-recordings and daylong audio-recordings within the same group of 44 infants at 6 and 7 months. We compared 12 measures of language quantity and lexical diversity, talker variability, utterance-type, and object presence, finding moderate correlations across recording-types. However, video-recordings generally featured far denser noun input across these measures compared to the daylong audio-recordings, more akin to ‘peak’ audio hours (though not as high in talkers and word-types). Although audio-recordings captured ~10 times more awake-time than videos, the noun input in them was only 2--4 times greater. Notably, whether we compared videos to daylong audio-recordings or peak audio times, videos featured relatively fewer declaratives and more questions; furthermore, the most common video-recorded nouns were less consistent across families than the top audio-recording nouns were. Thus, hour-long videos and daylong audio-recordings revealed fairly divergent pictures of the language infants hear and learn from in their daily lives. We suggest short video-recordings provide a ‘dense and somewhat different’ sample of infants’ language experiences, rather than a ‘typical’ one, and should be used cautiously for extrapolation about common words, talkers, utterance-types, and contexts at larger timescales. If theories of language development are to be held accountable to 'facts on the ground' from observational data, greater care is needed to unpack the ramifications of sampling methods of early language input.
   
keywords          : "language acquisition, naturalistic observational data, infants, early home environment, language input, cognitive development"
wordcount         : "3949"

bibliography      : ["sixseven.bib", "r-references.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---

```{r read in data, comment=F, message=F, hide = T, warning = F, echo = F}
library("papaja")
library("knitr")
source("sixseven_data_aggregation.R")
source("same_top_combo.R")
#source("wilcoxon_corrs_sixseven.R")
#source("sixseven_figure_code.R")
source("sixseven_simplestats.R")
source("sixseven_simplegraphs.R")
source("sixseven_visit_ages_table.R")
source("sixseven_measures_table.R")
source("sixseven_vboost_table.R")
source("sixseven_reliability.R")


getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

#to do:
# * add author contribution note? something like this?
  #EB designed the research, analyzed the data, and drafted the paper. 
  #SD, SK, and ST collected and annotated/oversaw annotation of the data. 
  #AA wrote code-base for data annotation pipeline. AA, SD, and SK contributed to data aggregation and manuscript code. 
  #All authors contributed to writing. 

```

```{r analysis_preferences}
# Seed for random number generation
set.seed(42)
```

# Highlights
  * We measured 44 infants’ early noun input during free-form interactions in hour-long videos and daylong audio-recordings; sampling approach shifted potential conclusions about home language environment.
  * Across quantity, utterance-type, object presence, and talker measures, nouns-per-minute were 2--4 times more frequent in video- than audio-recordings; video hours largely mirrored peak audio hours.
  * Nouns in videos occurred relatively more often in questions and less often in declaratives than they did in daylong or peak audio-recording hours
  * The most frequent nouns in daylong and peak audio-recording hours highly overlapped in identity and across families; this was less true for top video nouns.

Researchers have studied development by observing infants in their natural habitats for decades [@taine1876note; @williams1937analytical]. Over the past 20--30 years, written records have been increasingly supplemented with audio- and video-recordings, depicting infants' linguistic, social, and physical landscape. Such data --- often shared through repositories like CHILDES and Databrary --- in turn provide a proxy for various 'input' measures in theories of social, motor, and particularly, *linguistic* development [@macwhinney2001emergentist]. 

Furthermore, recent technological advances allow the collection of longer, denser, and higher-quality recordings, used to study infants' input and language skills [@bergelson2017nature; @roy2015predicting; @oller2010automated; @vandam2016homebank; @weisleder2013talking, *inter alia*]. Such naturalistic data seeks to reveal what infants actually learn from as they make use of their biological endowments and environmental resources.
 
While improving technology makes collecting observational data ever easier, more alternatives create more decision-points, with serious but underexplored side-effects. Researchers must decide on recording modalities (e.g. audio, video, both), where, whom, and how long to record, and whether to capture structured or free-ranging interactions, with or without experimenters present. While any path through such decision-trees may lead to equivalent results, this is rarely tested. Problematically, this leads to research with *theoretical* conclusions built on unmeasured *methodological* equivalence assumptions.
 
In recent work directly comparing sampling methods, @tamis2017power analyzed mother-infant behavior in 5-minute structured interactions and 45 minutes of free play at home. They found while language quantity across contexts correlated,  relative to free play, infants experienced more words per minute (both types and tokens) in structured interactions. They conclude that sampling must be matched with research question, cautioning that while brief samples may be appropriate for studying individual differences, extrapolations from short samples must be made with care. 

In contrast, work by @hart1995meaningful extrapolated extensively. Based on 30 hours of data per family (collected one hour per month for 2.5 years), these researchers estimated that by age four, children receiving public assistance (n=6) heard >30 million fewer words than professional-class children (n=13). While their findings highlighting SES differences certainly merited (and received) follow-up [e.g. @fernald2013ses; @noble2005neurocognitive, *inter alia*], the results have also been criticized as an extreme over-extrapolation [@dudley2009pathologizing; @michaels2013commentary].

Still other research analyzes base rates of certain linguistic phenomena to provide data reflecting what young children hear [@brent2001role; @tomasello2000young; @lidz2003infants]. Unfortunately, it is difficult to predetermine an 'appropriate' sample for such base rates. For instance, practically any length of adult speech, across wide-ranging recording parameters, will find function words (e.g. 'of') at much higher rates than content words (e.g. 'fork'). For questions concerning many aspects of infants' language input, however, it is largely unknown how sampling may bias results, and thus various practical constraints on data collection and availability often guide sampling decisions.

Here we explore sampling directly, comparing hour-long video-recordings and daylong audio-recordings in a single sample of 44 infants, as part of a larger study on early noun learning. We annotated concrete nouns said to (or loudly and clearly near) infants. We further annotated three properties previously linked with early language learning: (1) utterance-type, which provides syntactic/situational information [@hoff2002children; @brent2001role; @debaryshe1993joint] (2) object presence (i.e. referential transparency), which clarifies whether spoken words' referents are visually appreciable [@bergelson2017nature; @bergelson2013acquisition; @yurovsky2013statistical; @cartmill2013quality], and (3) talker, which measures speaker quantity and prevalence [@rost2010finding; @bergmann2016discriminability].

This design sets up two overarching questions. First, does noun input in one video-recorded hour predict noun input in an entire audio-recorded day? Second, do input quantities differ once time is normalized? If the noun input is equivalent and predictive across recording-types, then researchers can freely vary observational data collection approaches with impunity. If not, understanding methodological biases is critical to ensuring that learning theories consider the data quantity and variability typically available to learners.  

Below we compare language input across four key properties (word quantity/diversity,  utterance-type, object presence, and talker), as measured by hour-long videos and (separate) full-day audio-recordings. This seemingly methodological research has deep implications for developmental theory: we examine how sampling may alter conclusions about the linguistic input driving early development.

# Methods
## Participants
Infants were recruited from a database of families from local hospitals, or through BabyLab outreach. Forty-six participants enrolled; two dropped out leaving 44 in the final sample. All were full-term (40±3 weeks), had no known vision or hearing problems, and heard $\geq 75\%$ spoken English. Participants were 95% white; 75% of mothers had $\geq$B.A. The families were enrolled in a yearlong study that included monthly audio- and video-recordings, as well as in-lab visits every other month. See Table \@ref(tab:recording-ages-table) for age details. Here we report on the home recording data from the first two timepoints (6 and 7 months) of this study, for which participants received $10.\footnote{We include only these timepoints because infants had not yet begun producing words themselves (which changes the input). Given the broader project aims, these timepoints alone had the entire daylong audio-recording annotated.}  

## Procedures
Participants gave consent at an initial lab visit for the larger study through a process approved by university IRB. Questionnaires concerning family/infant background, not germane to the present analysis, are reported elsewhere [@bergelson2017nature; @Laing_Bergelson_17]. Four recordings are analyzed for each infant: an audio- and video-recording at 6 and at 7 months, each on a different day\footnote{A video is missing for one infant due to technical error.}. See Table \@ref(tab:recording-ages-table). Parents who signed release forms allowing their recordings to be shared with other researchers are available via Databrary. 

## Video-Recordings
Researchers visited infants' homes each month to video-record an (ostensibly) typical hour of infants' lives. Infants were outfitted with a hat or headband affixed with two small Looxcie cameras (22g each). One camera was oriented slightly down and the other slightly up, to best capture infant's visual field (verified via Bluetooth with an iPad/iPhone during setup). A standard camcorder (Panasonic HC-V100 or Sony HDR-CX240) was positioned in the corner on a lightweight tripod which parents were asked to move if they changed rooms. After set-up, experimenters left for one hour.

## Audio-Recordings
Audio-recordings captured up to 16 hours of infants' input. Parents were given LENAs (LENA Foundation, Boulder, CO), small audio-recorders (<60g) along with infant vests with a LENA-sized chest pocket. Parents were asked to put the vest and recorder on babies from when they awoke to when they went to bed (excepting naps and baths). Parents were permitted to pause the recorder anytime but were asked to minimize such pauses.

```{r lengths, echo = F, warning = F, message = F}
vidmode <- getmode(round(vidtime$total_min)) #rounding to the minute 1st
audmode <- getmode(audtime$total_min)
audmode_nosil_hr <- round(getmode(audtime$tot_nosil/60),1)
aud_nosilM <- round(mean(audtime$tot_nosil)/60,2)
aud_nosilSD <- round(sd(audtime$tot_nosil)/60,2)
aud_nosilmin <- round(min(audtime$tot_nosil)/60,2)
aud_nosilmax <- round(max(audtime$tot_nosil)/60,2)

vidM_SD_R <- paste(round(mean(vidtime$total_min),2), "min., SD=",round(sd(vidtime$total_min),2),
                   ", R=",min(vidtime$total_min), "--", max(vidtime$total_min),"min.", sep = "")
audM_SD_R <- paste(round(mean(audtime$total_hr),2), "min., SD=",round(sd(audtime$total_hr),2),
                   ", R=",min(audtime$total_hr), "--", max(audtime$total_hr)," hr", sep = "")
audM_SD_Rmin <- paste(round(mean(audtime$total_min),2), "min., SD=",round(sd(audtime$total_min),2),
                   ", R=",min(audtime$total_min), "--", max(audtime$total_min),"min.", sep = "")

audnosilM_SD_R <- paste(aud_nosilM*60, "min., SD=",aud_nosilSD*60,", R=",aud_nosilmin*60, "--", aud_nosilmax*60, "min.", sep = "")
```

## Data Processing
Details of the entire data-processing pipeline are on OSF (https://osf.io/cxwyz/wiki/home/). Videos were processed using Vegas and in-house video-editing scripts. Footage was aligned in a single, multi-camera view before manual language annotation in Datavyu. Audio-recordings were initially processed by LENA proprietary software, which segments and diarizes each audio file; this output was then converted to CLAN format [@macwhinney2010transcribing]. Through in-house scripts, long periods of silence were marked in these CLAN files (e.g. naptime), which were then used for manual language annotation. 

Modally, videos were an hour (`r vidmode`min., *M*=`r vidM_SD_R`), and audio-recordings were 16 hours (`r audmode`min., *M*=`r audM_SD_Rmin`), the maximum capacity of the LENA. Removing the long silences from audio-recording left ~10hrs. of audio (Mode=`r audmode_nosil_hr*60`min., *M*=`r audnosilM_SD_R`). Approximately 10 hours of awake-time during the day is inline with established norms for 6--8-month-olds in the U.S. [@mindell2010cross]. All infants were awake for the entire video-recording except one, whose video annotation ended at sleep onset. 

## Language Annotation
Trained researchers annotated each recording. This entailed demarcating each concrete noun directed to or easily overheard by the child (e.g. words directed at adjacent siblings), but not distant language (e.g. background television).'Object words' were operationalized as concrete, imageable nouns (e.g. shoe, arm). Each annotation noted the word and lemma (e.g. teethies, tooth), along with *utterance-type*, *object presence*, and *talker*. For *utterance-type*, each object word's utterance was classified as declarative, question, imperative, reading, singing, short-phrase, or unclear. (Short-phrase utterances included words in isolation and <3 word noun-phrases, e.g. 'the red ball' or 'kitty's paw'.) *Object-presence* coded whether objects were present and attended to (yes/no) based on linguistic context (e.g. 'here's your spoon!' was scored 'yes' for object presence); for videos visual context was also used. Lastly, *talker* tagged live interlocutors and electronics: mother, toy, etc.; this was checked by staff highly familiar with each family. We assessed intercoder reliability on a random contiguous 10% of the annotations in each file for the two categorical variables (utterance-type and object-presence). Reliability was moderate to strong (utterance-type: `r meanagreeUT`% agreement, Cohen's $\kappa$=`r meankappaUT`; object-presence: `r meanagreeOP`% agreement, Cohen's $\kappa$=`r meankappaOP`).

# Results
## Analysis Plan
Based on the coding scheme above, we derived 12 measures from each recordings' annotations for each child (n=44), recording-type (audio, video), and month (6, 7). See Table \@ref(tab:measures-tab). We averaged across months to increase precision, and because we have no theoretically-motivated reason to predict input differences across months (i.e. no developmental or linguistic milestones are typically achieved at 6--7 months.) While we initially anticipated analyzing the data with multi-level models, nearly all such models revealed highly skewed residuals (by Shapiro-Wilk Test), even when log-transformed, limiting interpretation across measures. Thus, we instead report a simple set of nonparametric analyses below. We used R for all analyses; the code that rendered this manuscript is on Github, to be shared upon publication.\footnote{Please contact corresponding author for access before publication.}

For all recording-type comparisons, we look at whether our measures *differed* significantly (by two-tailed, paired Wilcoxon Test) and *correlated* significantly (by Kendall Rank Correlation) across the given groups. This approach lets us compare, e.g., whether the time-normalized count of declarative nouns is indistinguishable in our audio- and video-recordings, independently of whether these values are correlated. We applied Holm's *p*-value adjustment for multiple comparisons [@holm1979simple] for the set of 12 Wilcoxon tests and the set of 12 Kendall Correlations.

```{r scaling examination, echo = F, warning = F, message = F}
mean_vboost_measures <- vboost_mean_collapsed %>% 
  summarise(mean_measureboost = mean(vboost_types:vboost_op)) %>% as.double()
```

## Count Measure Analysis
To examine how the hour-long video data 'scale' to day-length data descriptively, we first divided the 12 count measures from the videos by those from the audio-recordings for each child, to derive 'video-fraction' scores (video/audio). This showed that the video-recordings were `r vboost_mean_collapsed$vboost_min` of the length of audio-recordings, or `r vboost_mean_collapsed$vboost_awakemin` of the length when audio-recording silences were removed. However, rather than a concomitant 10-fold decrease in our count measures (as would be expected if videos captured a 'representative' hour of the day), the fractions averaged to `r mean_vboost_measures`; see Table \@ref(tab:normtable). Thus, by and large, videos had a denser concentration of nouns across measures than did the audio-recordings. See Figure \@ref(fig:gr-derived-counts-67-diff) for raw count data for each measure. 

We computed video-fractions (rather than the reciprocal, i.e. audio/video) because there were more zero values for videos than audio-recordings (e.g. children who heard no sung nouns), rendering more undefined values. Indeed, >1/3 of children did not hear nouns in reading or from fathers on videos in either month. See Table \@ref(tab:propna-missing-tables). 

We next normed our counts by the number of minutes in each recording. E.g., if an infant heard 500 noun-tokens in 800 minutes of non-silent audio-recording, and 200 in 60 minutes of video, this was normed to .62 and 3.3 noun-tokens per minute, respectively; zero values were retained within normed counts.\footnote{One infant's data was excluded from 'father' measures; this infant had no father at home.}

```{r wilcoxon-corr-stats, warning = F, echo = F, message = F}
w_sigps <- nrow(ws %>% filter(pval_adj<.05))
c_sigps <- nrow(cs %>% filter(pval_adj<.05))
tau_min <- round(min(c_taus_sig$estimate),2)
tau_max <- round(max(c_taus_sig$estimate),2)
tau_mean <- round(mean(c_taus_sig$estimate),2)

s_t_overlap <- top_same_overlappers %>% filter(!SubjectNumber %in% c("25_07","12_06","37_06")) %>% summarise(s_t_overlap=n_distinct(SubjectNumber))
SHw_sigps <- nrow(SHws %>% filter(pval_adj<.05))
THw_sigps <- nrow(THws %>% filter(pval_adj<.05))

```

We first looked at correlations across recording types, and find that `r c_sigps`/12 metrics correlated in audio vs. video data; nouns per minute heard from fathers and in singing did not. The size of the correlations (i.e. Kendall's $\tau$) was moderate (excluding the two non-significant metrics, *M*=`r tau_mean`, `r tau_min`-`r tau_max`, all adjusted-*p*<.05). See Table \@ref(tab:normtable) and Figure \@ref(fig:gr-derived-normcounts-corr). 

We next compared the rates of each measure in three ways. First, we used the normed data, looking at counts per minute. With the normed data, `r w_sigps`/12 metrics occurred at significantly lower rates in audio-recordings than video-recordings (all adjusted-*p*<.05). The remaining metric, nouns from fathers, was statistically indistinguishable across recording types (adjusted-*p*>.05). Thus, overall, per unit time, infants heard less noun input across our metrics of quantity, talker, utterance-type and object presence in audio-recordings than in videos (see Figure \@ref(fig:gr-derived-counts-67-diff) and \@ref(fig:gr-derived-normcounts-diff) for raw and normed count data).

Next, we compared two different hour-long subsets from the daylong audio recording for comparison with the video-recorded hour, collectively referred to as 'peak' audio times. The *top* hour was calculated by sliding a window through the annotations to find the hour in which infants heard the most nouns. Complementarily, under the logic that parents scheduled the video-recordings for times of high infant altertness, we extracted that *same* hour in the daylong audio, i.e. if the video recording visit was scheduled from 2-3pm, we used 2-3pm from that child's daylong audio recording that month. Our 12 measures were then computed in both the *top* and *same-as-video* hours of audio. Top and same-as-video hours only overlapped in `r s_t_overlap$s_t_overlap`/88 recordings (`r round(s_t_overlap$s_t_overlap/88,2)*100`%)).\footnote{In 3 cases, the video-recording time (i.e. 'same-as-video' time) preceded the beginning of the daylong audio-recording (by 5, 30, or 90 minutes); in those cases the first hour of the recording was used. This created two further cases of \'top' and 'same-as-video' overlap.}

For the peak audio, `r SHw_sigps`/12 metrics occurred at significantly *higher* rates in 'same' audio hour than in video-recordings (all adjusted-*p*<.05), namely the number of types, nouns from fathers, and nouns from declaratives. Similarly, `r THw_sigps` occurred at significantly higher rates in 'top' audio hour than in video-recordings (all adjusted-*p*<.05); these included all the measures that were higher in the 'same' audio hour, along with number of tokens, and nouns in imperatives and short phrases. Thus, the video data presented a somewhat different language input profile than the peak audio hours of the day: they featured less input for some of our quantity, talker, and utterance-type measures, but were statistically indistinguishable in object presence, input from mothers, and input in other utterance-types. This same qualitative pattern held when looking at 'zero' values for the peak audio hours, relative to the videos and daylong audio-recordings (see Table \@ref(tab:propna-missing-tables)).


```{r prop-ut, echo = F, warning = F, message = F}
dq_propmeans <- sixseven_spreadAV_collapsed %>% 
  dplyr::select(a_propd, v_propd, a_propq, v_propq) %>% 
  summarise_all(.funs = mean)
THdq_propmeans <- TOPHOURsixseven_spreadAV_collapsed %>% 
  dplyr::select(a_propd, a_propq) %>% 
  summarise_all(.funs = mean)
SHdq_propmeans <- SAMEHOURsixseven_spreadAV_collapsed %>% 
  dplyr::select(a_propd, a_propq) %>% 
  summarise_all(.funs = mean)
#pvalues for d&q here:
#propws%>% filter(pval_adj<.05)
#SHpropws %>% filter(pval_adj<.05)
#THpropws%>% filter(pval_adj<.05)
```  

## Exploratory Analyses
Lastly, we undertook two sets of highly exploratory analyses, at the utterance and word level. The utterance-type analysis is based on the unanticipated result that while declaratives and questions made up >2/3 of the input for each recording-type, the videos appeared to contain relatively more questions and fewer declaratives (See Fig. \@ref(fig:gr-derived-counts-67-diff) and \@ref(fig:gr-derived-normcounts-diff)). To test this statistically, we converted the six utterance-type counts to proportions (e.g. number of nouns heard in declaratives over total noun tokens), and compared the proportion of each utterance types in video- and audio-recordings. Wilcoxon tests of each utterance-type in audio- vs. video-recording (corrected for multiple comparisons) revealed that indeed, declaratives and questions occurred at different rates across recording-types, with audio-recordings containing relatively fewer questions ($M_{video}$=`r dq_propmeans$v_propq`, $M_{audio}$=`r dq_propmeans$a_propq`, $M_{same\  audio}$=`r SHdq_propmeans$a_propq`, $M_{top\  audio}$=`r THdq_propmeans$a_propq`) and more declaratives than videos ($M_{video}$=`r dq_propmeans$v_propd`, $M_{audio}$=`r dq_propmeans$a_propd`, $M_{same\  audio}$=`r SHdq_propmeans$a_propd`, $M_{top\ audio}$=`r THdq_propmeans$a_propd`; each video vs. audio comparison adjusted-*p*<.05). No other proportional utterance-type differences reached significance across recording-types (all adjusted-*p*>.05). See Figure \@ref(fig:gr-ut-count-collapsed). 

At the word level, we aimed to characterize whether audio- and video-recordings captured the same nouns and the same relative frequencies across words and families. Nouns' frequency distribution was Zipfian: of the `r totaln_objectwords` unique object words (`r totaln_bl` lemmas) heard across months and recording-types, only `r totaln_once_objectwords` (`r totaln_once_bl` lemmas) occurred more than once.

```{r top-words, warning=F, message= F, echo = F}
avtop100_numwords <- top100av_spread %>% nrow()
avtop100corr_est<- round(cor_AVtop$estimate,2)
avtop100corr_pval<- ifelse(cor_AVtop$p.value<.0001, "*p*<.0001", "fix-this")
avtop100_numwordsnozeros <- top100av_spread_nozeros %>% nrow()
avtop100corr_estnozeros <- round(cor_AVtopnozeroes$estimate,2)
avtop100corr_pvalnozeros<- ifelse(cor_AVtopnozeroes$p.value<.0001, "*p*<.0001", "fix-this")

audio_nfams <- overall_top10_nfams %>% filter(audio_video=="audio")
video_nfams <- overall_top10_nfams %>% filter(audio_video=="video")

```
 
We examined the 100 most frequent nouns from audio- and video-recordings (n=`r avtop100_numwords` due to ties, n=`r avtop100_numwordsnozeros` without words that occurred zero times in one recording-type). Frequency across recording-types correlated significantly (Kendall's tau: `r avtop100corr_estnozeros`, `r avtop100corr_pvalnozeros`) even with zero-frequency words included (Kendall's tau: `r avtop100corr_est`, `r avtop100corr_pval`; see Figure \@ref(fig:top-100-logspace) and \@ref(fig:top100-corr-rectype) ). 

Finally, we analyzed the top ten words by recording-type. Four of the top ten words in each recording-type overlapped (baby, book, mouth, toes), suggesting that extremely common words are relatively conserved across recording-types. However, the top audio words were far more common across families (see Figure \@ref(fig:top10noun-freq)). Indeed, the ten most frequent nouns in audio-recordings were heard by `r audio_nfams$min_nfams`-`r audio_nfams$max_nfams` (*M*=`r audio_nfams$mean_nfams`(`r audio_nfams$sd_nfams`)) of the 44 families; those in video-recordings were heard by `r video_nfams$min_nfams`-`r video_nfams$max_nfams` (*M*=`r video_nfams$mean_nfams`(`r video_nfams$sd_nfams`)). Finally, the top audio words were ~3x as common as the top video words ($M_{audio}$=`r audio_nfams$mean_freq`(`r audio_nfams$sd_freq`), $M_{video}$=`r video_nfams$mean_freq`(`r video_nfams$sd_freq`)), again underscoring the higher density of nouns in video-recordings (which were ~1/10 the length of audio-recordings). Taken together, this exploratory analysis suggests that daylong audio-recordings may render more stable estimates of pervasively common words across families than do video-recordings. 
 
#Discussion
Our results can be distilled to three key findings. First, infants heard relatively more nouns in videos than in the audio-recordings. Per minute, infants heard ~2--4x more noun input across quantity, speaker, utterance-type, and object-presence metrics when video-recorded for an hour versus audio-recorded for a day. Second, while our metrics generally correlated across audio- and video-recordings, the relative rates of the most prevalent utterance-types and the rates of unattested data for certain metrics varied across them. Finally, while the highest frequency words across recording types largely overlapped and correlated, top words from the daylong audio-recording appear to better represent the noun input across families.

## Noun Quantity and Lexical Diversity
Overall, the pattern across recording-types primarily suggests a difference in volubility, since by-and-large measures both correlated and differed across recording-type. As Suskind et al.[-@suskind2013exploratory] noted about an intervention, daylong audio recordings likely provide more realistic counterparts to 'best behavior' hourlong video-sessions. We add that shorter video-recording itself may elicit higher speech-volume, separate from caretakers' deliberate intent.

Indeed, families likely found it simply easier to behave freely with infants in special vests than with cameras on their heads. *Apropos* equipment prominence, both ‘hat’ and ‘camera’ were top-10 video words; no analogous nouns (e.g. vest, recorder) topped the frequency rankings in audio-recordings (see Figures \@ref(fig:top-100-logspace) and \@ref(fig:top10noun-freq)).

Our comparison across recording-types highlighted many differences across measures, even with family and age held constant. The quantity metrics provide a conceptual replication and extension of @tamis2017power. Despite numerous methodological differences (length/type of recordings compared, experimenter presence, age, word-class analyzed), both studies found that parent talk per unit time was significantly higher in shorter recordings. While the difference they find is less extreme numerically (roughly 1.5--2x the number of types and tokens in the longer vs. shorter recording compared to our 2--3x difference), this general pattern appears robust across our sampling methods. Taken together, this suggests that shorter recordings elicit denser caregiver talk.

For certain research questions, these differences in volubility and lexical diversity may not matter. E.g. for studies examining relative rates of word use and object interactions during a concentrated in-lab exposure and test phase, denser talk in shorter recordings may be less relevant. In contrast, research quantifying language input across populations with varying demographic, social, and cultural properties may need to be particularly sensitive to cross-sample comparison [cf. @bergelsonunderreview; @cristia2017child; @shneidman2012language].

##Object Presence
Object presence was higher in videos than in audio-recordings. This may be because the video-recordings truly had more object presence (i.e. infants mostly stayed in 1--2 rooms, interacting with what was at hand). Alternatively, it may be the case that there are more ambiguous cases of ‘object presence’ in audio-recordings than video-recordings, which may have contributed to ‘not present’ annotations at higher rates. Indeed, although object presence did correlate significantly across recording-types (`r subset(c_taus_sig,comp=="c_yop")$estimate`), inter-rater reliability was higher for videos than audio-recordings for this measure (audio: `r agreeaudOP$value`% agreement, Cohen's $\kappa$=`r kappaaudOP$value`; video: `r agreevidOP$value`% agreement, Cohen's $\kappa$=`r kappavidOP$value`). Our interpretation is that both factors are likely at play, i.e. that the object-presence difference we find reflects a true difference between situations that arise during daylong audio vs. hour-long video-recordings, and that there is more noise in the estimate of object-presence when visual information is unavailable. Given that object presence and the related ideas of referential transparency and contingent talk have been linked with early language development [@bergelson2017nature; @yurovsky2013statistical; @mcgillion2017randomised; @cartmill2013quality], this property merits follow-up. A better understanding of  situations and contexts that elicit contingent, referentially-transparent caretaker talk (around objects or otherwise) may be a fruitful avenue for further work.

##Talker Variability  
Infants heard nouns from more talkers per minute in videos than in audio-recordings, though in raw numbers infants heard roughly double the speakers over the course of a day as they heard in one video-recorded hour (see Figure \@ref(fig:gr-derived-counts-67-diff)). While we considered noun input from all sources (human, electronic, etc.), the quantity of talkers was largely swamped by input from mothers (~65%), which was also greater in videos than audio-recordings. The proportion of input from fathers did not vary by recording-type, though over half of videos did not include noun input from fathers at all. This is largely due to sample demographics: video-recording took place during weekday business hours, when the fathers in this sample were largely at work. In contrast, audio-recordings spanned work-hours and days of the week. Given that fathers and mothers make different contributions to early language development [@pancsofar2006mother], this is a clear example of a consequence of methodological choices: to better understand parents' input, considering work-schedules is critical.

Talker variability is also relevant for recent in-lab studies: while infants at the same age tested here looked equivalently to named images when words were produced by a new person or their mother [@bergelson2017young], slightly older infants showed a word-learning advantage when multiple talkers name new objects [@rost2010finding]. Furthermore, talker variability differentally influences certain phonetic discriminations [@bergmann2016discriminability]. Indeed, viable models of phonetic learning must encorporate mechanisms specifying the role of talker variability, even if only to show that a large range of talker input converges on effectively equivalent phonetic categories. Better approximations of infants' quotidian experiences are a prerequisite for such models. The present results indicate that such estimates of talker variability are inflated in hour-long videos relative to daylong audio-recordings.  

## Utterance-Types  
We found more nouns in every utterance-type in videos than in audio-recordings, per unit time. These utterance-types were a mix of largely syntactic constructions (declaratives, questions, imperatives, short phrases) and more situationally-defined utterance-types (reading, singing). While its not particularly surprising that reading or singing rates might vary across recording-types, we did not anticipate differences in declaratives and questions, which made up most of the input. Indeed, while questions and declaratives made up the majority of the input for each recording-type, videos had relatively more questions and fewer declaratives. This is key instance where methodological choices may influence language acquisition theories: base-rates of questions taken from videos would inflate estimates of auxiliary verbs in the early input. Notably, previous work has found that studies vary in whether they find links between questions (yes/no and wh-) in the input and children’s early productions, invoking developmental level to explain cross-study differences [@barnes1983characteristics; cf. @huttenlocher2002language]. We add the possibility that recording-type too may contribute to the base-rates of questions in the input, even with age kept constant.


## Top Words
Our interpretation of these results is that relatively short video-recordings overestimate young infants’ typical noun input, and that extrapolation based on daylong audio-recordings likely better represents infants’ daily lives. This underscores our third main finding: the conclusions one would draw about which words are most common in young infants’ language input differ in their robustness across families by recording-type. That is, the top audio words were all heard by $\geq75\%$ of these families; only one of the top 10 video words ('hat') was this common, and was clearly tied to the recording equipment (see Figure \@ref(fig:top10noun-freq)). This result may be meaningful in several ways. First, corpora of child language input offer our best proxies for what infants learn from: our 'top words' analysis suggests that the input would seem far more heterogeneous across children based on hour-long video-recordings alone. Second, word frequency and prevalence across families are often used to select stimuli for in-lab study; relying on estimates from shorter, less representative recordings may stymie the words studied in the lab. Thus, understanding how cross-family noun-input stability scales with recording-length may prove critical for future research; the word-level results above are an initial exploration in understanding this dimension of naturalistic observational data.

## Limitations and Conclusions
Given the technical limitation that currently available small video-recorders have a shorter battery-life than audio-recorders, we cannot conclusively separate the effects of modality and length. That is, had we only audio-recorded for an hour or recorded video all day, we may have obtained equivalent results across recording modalities. Indeed, sub-sampling a single hour from each audio-recording would not be an appropriate comparison to the videos, given that this hour would either be constrained by time of day in a way the videos were not, or if random, may contain features wholly different from the video recordings, e.g. naps or travel in the car, and the absence of researchers and equipment bookending the session. More direct comparisons awaits technological progress. A further limitation is self-selection into the study: many parents are unwilling to invite researchers to record their infants. Relatedly, our convenience sample does not reflect the broader demographics of the US (let alone other cultures), and as such, should be extended to other populations before conclusive generalizations about sampling methodology can be made [cf. @bergelsonunderreview].

Understanding what infants learn from is a key part of understanding what and how they learn at all. Here we have taken first steps in understanding how two different data collection approaches may influence our conclusions about early linguistic input. We find that even naturalistic observer-free video-recordings appear to inflate language input relative to daylong recordings, in ways that influence syntactic constructions, word-specific experiences, talker-variability, and the sheer quantity and diversity of nouns infants hear. Work from the preceding decades suggests all of these factors matter for early learning. Yet without knowing how sampling methods may hamper us in principle, we necessarily limit our ability to adequately model infant language acquisition. The present work charts datapoints within this largely underspecified space, probing how robust linguistically-relevant measures are across naturalistic sampling methods of infants' everyday experiences.\newpage
```{r create_r-references}
r_refs(file = "r-references.bib")
```

```{r recording-ages-table, comment=F, message=F, hide = T, warning = F, echo = F, results = "asis"}
apa_table(ages_table_data, caption = "Infant ages at home recordings and enrollment lab visit")
```
\pagebreak
```{r measures-tab, echo = F, warning = F, message = F, results = "asis"}
apa_table(measures_table_data2,caption = "Count measures (n=12), by Measure-Type",small = T)
```
\pagebreak
```{r propna-missing-tables, echo = F, warning = F, message = F, results = "asis"}
apa_table(all_vars_with_NAs, caption = "Proportion of infants with no recorded nouns for the listed measures, by sample", small = T, note = "V=video, A=daylong audio, Top=top hour of A, Same = Video-hour of A. All infants heard nouns for all other measures (see Table 2).")
```
\pagebreak
```{r normtable, echo = F, warning = F, results = "asis"}
apa_table(countvals_normed_vboost_table,cap="Video/Audio Count Measures, normed by minutes in recording (column 2) and divided without norming (column 3)", small = T, note = "If videos contained equivalent quantities of nouns, Inflation values would be 1, and Video-fractions would be .1")
```

```{r gr-derived-counts-67-diff, fig.cap = cap, echo = F, warning = F, fig.height=5, fig.width=7.5}
gr_countvals_long_collapsedNEW
cap <- "Noun count measures across audio-recordings and videos. Top row depicts daylong audio data; bottom row shows the 3 hour-long annotations: 'same' and 'top' are the two peak audio times, and 'video' indicates the video data. Upper panel labels indicate annotated sample length (day or hour); the bottom panel labels reflects measure type (op = object presence; utt = utterance-type, quant = quantity, Nspeakers = number of speakers). Bars (left to right) appear in legend order (top to bottom) in both color (count measures) and opacity (time sample: day, top-hour, same-hour, or video)."
```

```{r gr-derived-normcounts-diff, fig.cap = cap, echo = F, warning = F, fig.height = 5, fig.width =5.5}
gr_countvals_long_norm_collapsed
cap <- "Noun count measures normalized by recording length, for audio-recordings (solid borders) and videos (dashed borders). Normalized counts were calculated by dividing raw counts (see Fig 1.) by non-silent recording minutes. op = object presence; utt = utterance-type, quant = quantity, Nspeakers = number of speakers. Bars (left to right) appear in legend order (top to bottom). All measures differed significantly across recording-types except nouns from fathers."
```

```{r gr-derived-normcounts-corr, fig.cap = cap, echo = F,  fig.height= 3, warning = F}
gr_count_cor_VA_facetmonth_norm_collapsed
cap <- "Normalized count correlations between audio- vs. video-recordings. Each point indicates nouns per minute of recording for each child, averaged across months 6 and 7, for each measure. Point-shape indicates measure type. Robust linear correlations are plotted for visualization only; non-parametric correlations (Kendall) were computed for analysis, showing that all correlations were significant except nouns from fathers and in singing."
```

```{r gr-ut-count-collapsed, fig.cap = cap, echo = F,  fig.height = 3, fig.width = 4, warning = F}
gr_ut_count_collapsedNEW
cap <- "Utterance-type proportions across audio-recordings (daylong, 'same' hour and 'top' hour) and videos (indicated by line-type). Utterance-types are in legend order top to bottom. Videos contained a significantly more questions and fewer declaratives than the audio-recording time samples."
```

```{r top-100-logspace, fig.cap = cap, echo = F, warning = F, message= F, fig.width = 9, fig.height = 7}
top100_logspace_av_graph
cap <- "Log-scaled counts of the top 100 words in audio- and video-recordings. Each node represents the averaged count, across all participants in both months, of each noun (0.1 was added before taking logs to include 0 counts.) Words in blue occurred 0 times in one recording type; words in pink were attested in both recording types. Nodes are jittered for visual clarity, with grey lines indicating node location on axes."
```  

```{r top100-corr-rectype, fig.cap = cap, echo = F, fig.height = 3, fig.width = 3}
gr_top100_avspread_collapsed
cap <- "Correlations of the frequencies of the top 100 words in audio- vs. video-recordings. Each node represents one word averaged across all participants in both months."
```

```{r top10noun-freq, fig.cap = cap, echo = F, warning = F, fig.height = 3, fig.width = 8, fig.align="h"}
top10_graph_collapsedNEW
cap<-"Top 10 words by recording type and time sample. Each node represents the frequency count of each top audio or video word over both months (x-axis) and the number of families where that word was said (out of 44) across months (y-axis)."
```  

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
